{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212b4563",
   "metadata": {},
   "source": [
    "## Outcome prediction after Chikungunya hospitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a2939",
   "metadata": {},
   "source": [
    "#### MC853 - Unicamp\n",
    "\n",
    "- Leandro Henrique Silva Resende – 213437 \n",
    "\n",
    "- Pietro Grazzioli Golfeto – 223694 \n",
    "\n",
    "- Yvens Ian Prado Porto – 184031 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35229f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "# This script focuses on fairness analysis. It assumes that the data\n",
    "# has already been preprocessed (cleaned, imputed, scaled) and that the\n",
    "# training data has also been resampled (e.g., using SMOTE and RandomUnderSampler).\n",
    "# We used Python 3.10.12\n",
    "\n",
    "import pandas as pd # For data manipulation and analysis\n",
    "import os           # For operating system dependent functionalities like path handling\n",
    "import numpy as np  # For numerical operations\n",
    "\n",
    "# Define random state for reproducibility\n",
    "random_state = 42\n",
    "\n",
    "# Scikit-learn components for modeling and evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest classification model\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score # For model performance evaluation\n",
    "\n",
    "# Warnings management\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning) # Suppress DataConversionWarning for cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f943f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the PREPROCESSED and PRE-RESAMPLED dataset files.\n",
    "# These paths point to the CSV files that were generated by the separate training/preprocessing script.\n",
    "# They ensure that these file paths are correct and the files exist.\n",
    "\n",
    "# --- Data Usage Strategy ---\n",
    "# - `_resampled.csv` (X_train_fit_path, y_train_fit_path):\n",
    "#   These datasets are used exclusively for FITTING/TRAINING the machine learning models.\n",
    "#   The training features (X) and labels (y) in these files have undergone resampling\n",
    "#   to address class imbalance.\n",
    "#\n",
    "# - `_processed.csv` (X_train_eval_path, y_train_eval_path):\n",
    "#   These datasets contain the training data AFTER preprocessing (imputation, scaling, etc.)\n",
    "#   but BEFORE resampling. They represent the original distribution of the training data.\n",
    "#   They are used for EVALUATING fairness metrics on the training set because fairness should\n",
    "#   be assessed against the true underlying data distribution.\n",
    "#\n",
    "# - `_processed.csv` (X_test_eval_path, y_test_eval_path):\n",
    "#   These datasets contain the preprocessed test data. The test data is NEVER resampled.\n",
    "#   It's used for all TEST SET evaluations (performance and fairness) to get an unbiased\n",
    "#   estimate of how the model would perform on new, unseen data.\n",
    "\n",
    "leandro_path = {\n",
    "    'X_train_fit_path': '/home/leandro/Documentos/UNICAMP/MC853/DataSUS-Chikungunya-ML/datasets/X_train_resampled.csv',\n",
    "    'y_train_fit_path': '/home/leandro/Documentos/UNICAMP/MC853/DataSUS-Chikungunya-ML/datasets/y_train_resampled.csv',\n",
    "    'X_train_eval_path': '/home/leandro/Documentos/UNICAMP/MC853/DataSUS-Chikungunya-ML/datasets/X_train_processed.csv',\n",
    "    'y_train_eval_path': '/home/leandro/Documentos/UNICAMP/MC853/DataSUS-Chikungunya-ML/datasets/y_train_processed.csv',\n",
    "    'X_test_eval_path':  '/home/leandro/Documentos/UNICAMP/MC853/DataSUS-Chikungunya-ML/datasets/X_test_processed.csv',\n",
    "    'y_test_eval_path':  '/home/leandro/Documentos/UNICAMP/MC853/DataSUS-Chikungunya-ML/datasets/y_test_processed.csv',\n",
    "}\n",
    "\n",
    "pietro_path = {\n",
    "    'X_train_fit_path': '/home/pietro/Desktop/DataSUS-Chikungunya-ML/datasets/X_train_resampled.csv',\n",
    "    'y_train_fit_path': '/home/pietro/Desktop/DataSUS-Chikungunya-ML/datasets/y_train_resampled.csv',\n",
    "    'X_train_eval_path': '/home/pietro/Desktop/DataSUS-Chikungunya-ML/datasets/X_train_processed.csv',\n",
    "    'y_train_eval_path': '/home/pietro/Desktop/DataSUS-Chikungunya-ML/datasets/y_train_processed.csv',\n",
    "    'X_test_eval_path':  '/home/pietro/Desktop/DataSUS-Chikungunya-ML/datasets/X_test_processed.csv',\n",
    "    'y_test_eval_path':  '/home/pietro/Desktop/DataSUS-Chikungunya-ML/datasets/y_test_processed.csv',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9bad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pietro's path configuration.\n"
     ]
    }
   ],
   "source": [
    "# Set the active path configuration based on file availability.\n",
    "# This allows the script to be run by different users without manual path changes each time.\n",
    "path_config_fairness = None # Initialize with None\n",
    "\n",
    "# Check if all necessary files for a user's path configuration exist.\n",
    "# It's important to check for both fitting and evaluation files.\n",
    "if os.path.isfile(leandro_path['X_train_fit_path']) and os.path.isfile(leandro_path['X_train_eval_path']):\n",
    "    path_config_fairness = leandro_path # Use Leandro's path configuration\n",
    "    print(\"Using Leandro's path configuration.\")\n",
    "elif os.path.isfile(pietro_path['X_train_fit_path']) and os.path.isfile(pietro_path['X_train_eval_path']):\n",
    "    path_config_fairness = pietro_path # Use Pietro's path configuration\n",
    "    print(\"Using Pietro's path configuration.\")\n",
    "else:\n",
    "    # If the required files for either configuration are not found, raise an error.\n",
    "    raise Exception('Preprocessed/Resampled data paths not found. Please check paths and ensure the preprocessing script has been run and generated all necessary files.')\n",
    "\n",
    "# Define specific file path variables using the selected configuration.\n",
    "# `os.path.expanduser` resolves '~' to the user's home directory.\n",
    "\n",
    "# Paths for Model Fitting (uses resampled training data)\n",
    "X_train_fit_path = os.path.expanduser(path_config_fairness['X_train_fit_path'])\n",
    "y_train_fit_path = os.path.expanduser(path_config_fairness['y_train_fit_path'])\n",
    "\n",
    "# Paths for Training Set Evaluation (uses original preprocessed distribution)\n",
    "X_train_eval_path = os.path.expanduser(path_config_fairness['X_train_eval_path'])\n",
    "y_train_eval_path = os.path.expanduser(path_config_fairness['y_train_eval_path'])\n",
    "\n",
    "# Paths for Test Set Evaluation (uses original preprocessed distribution)\n",
    "X_test_eval_path = os.path.expanduser(path_config_fairness['X_test_eval_path'])\n",
    "y_test_eval_path = os.path.expanduser(path_config_fairness['y_test_eval_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698622aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for model fitting (resampled training data)...\n",
      "X_train_fit shape: (15584, 37), y_train_fit shape: (15584,)\n",
      "\n",
      "Loading data for training set evaluation (preprocessed, original distribution)...\n",
      "X_train_eval shape: (20983, 37), y_train_eval shape: (20983,)\n",
      "\n",
      "Loading data for test set evaluation (preprocessed)...\n",
      "X_test_eval shape: (10109, 37), y_test_eval shape: (10109,)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets from the specified CSV file paths.\n",
    "# `low_memory=False` is a pandas `read_csv` option that can help with type inference for large files with mixed data types.\n",
    "# `.squeeze(\"columns\")` converts a single-column DataFrame (which `read_csv` might produce for y_train/y_test) into a pandas Series.\n",
    "\n",
    "# --- Data for Model FITTING ---\n",
    "# These are the resampled training features and labels. Models will be trained on this data.\n",
    "print(\"Loading data for model fitting (resampled training data)...\")\n",
    "X_train_fit = pd.read_csv(X_train_fit_path, low_memory=False)\n",
    "y_train_fit = pd.read_csv(y_train_fit_path, low_memory=False).squeeze(\"columns\")\n",
    "print(f\"X_train_fit shape: {X_train_fit.shape}, y_train_fit shape: {y_train_fit.shape}\")\n",
    "\n",
    "# --- Data for EVALUATING on Training Set's Original Distribution ---\n",
    "# These are the preprocessed (but not resampled) training features and labels.\n",
    "# Used to assess fairness on the original training data characteristics.\n",
    "print(\"\\nLoading data for training set evaluation (preprocessed, original distribution)...\")\n",
    "X_train_eval = pd.read_csv(X_train_eval_path, low_memory=False)\n",
    "y_train_eval = pd.read_csv(y_train_eval_path, low_memory=False).squeeze(\"columns\")\n",
    "print(f\"X_train_eval shape: {X_train_eval.shape}, y_train_eval shape: {y_train_eval.shape}\")\n",
    "\n",
    "# --- Data for EVALUATING on Test Set ---\n",
    "# These are the preprocessed test features and labels.\n",
    "# Used for final model performance and fairness assessment on unseen data.\n",
    "print(\"\\nLoading data for test set evaluation (preprocessed)...\")\n",
    "X_test_eval = pd.read_csv(X_test_eval_path, low_memory=False)\n",
    "y_test_eval = pd.read_csv(y_test_eval_path, low_memory=False).squeeze(\"columns\")\n",
    "print(f\"X_test_eval shape: {X_test_eval.shape}, y_test_eval shape: {y_test_eval.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb9ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Note:\n",
    "# Preprocessing steps (like imputation, outlier removal, scaling) and the\n",
    "# resampling pipeline (SMOTE, RandomUnderSampler) are NO LONGER EXECUTED in this script.\n",
    "# This is because this script now loads data that has ALREADY undergone these transformations\n",
    "# in a separate, preceding script (the \"training code\" or \"preprocessing code\").\n",
    "# This makes the fairness analysis script cleaner and faster, focusing solely on its main task.\n",
    "# Imports for these removed steps (e.g., KNNImputer, RobustScaler, SMOTE, etc.) have also been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27279f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global constants for the analysis\n",
    "\n",
    "# Define the sensitive attribute\n",
    "# 'GENDER' contains binary values representing gender groups (0 for male, 1 for female).\n",
    "SENSITIVE_ATTR_NAME = 'GENDER' \n",
    "\n",
    "# Pre-defined best hyperparameters for the Random Forest model.\n",
    "# These parameters have been determined through a hyperparameter tuning process (GridSearchCV) in a previous stage.\n",
    "# Using fixed, optimized parameters ensures consistency in model training for fairness comparison.\n",
    "BEST_RF_MODEL_PARAMS = {\n",
    "    'class_weight': {0: 1, 1: 3},           # Assigns different weights to classes, helpful for imbalanced data. Here, class 1 is weighted more.\n",
    "    'max_depth': 10,                        # Maximum depth of individual trees in the forest. Controls complexity.\n",
    "    'min_samples_split': 30,                # Minimum number of samples required to split an internal node.\n",
    "    'n_estimators': 100,                    # Number of trees in the random forest.\n",
    "    'random_state': random_state,           # Seed for the random number generator, ensuring reproducibility of model training.\n",
    "    'n_jobs': -1                            # Use all available CPU cores for training, speeding up the process.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "367f4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def _prepare_features(df: pd.DataFrame, sensitive_attr_column_name: str, drop_sensitive_attr: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the feature set from a DataFrame by:\n",
    "    1. Removing any columns that might have been added from previous model predictions (those starting with 'y_pred_').\n",
    "    2. Optionally, dropping the column specified by `sensitive_attr_column_name`.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame from which to select/prepare features.\n",
    "        sensitive_attr_column_name (str): The name of the sensitive attribute column.\n",
    "        drop_sensitive_attr (bool, optional): If True, the sensitive attribute column will be removed.\n",
    "                                             Defaults to False (sensitive attribute is kept).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the selected base features.\n",
    "    \"\"\"\n",
    "    # Create a list of columns to drop. Start with any columns that are named like predictions from previous runs.\n",
    "    cols_to_drop = [c for c in df.columns if c.startswith('y_pred_')]\n",
    "    \n",
    "    # If the `drop_sensitive_attr` flag is True, add the sensitive attribute column to the list of columns to drop.\n",
    "    if drop_sensitive_attr:\n",
    "        cols_to_drop.append(sensitive_attr_column_name)\n",
    "        \n",
    "    # Drop the identified columns from the DataFrame. A copy is made implicitly by `drop`.\n",
    "    # `errors='ignore'` prevents errors if a column in `cols_to_drop` isn't found.\n",
    "    return df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "def calculate_equal_opportunity(y_true: pd.Series,\n",
    "                                y_pred_values: np.ndarray,\n",
    "                                sensitive_attribute_col_values: pd.Series) -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculates the Equal Opportunity metric.\n",
    "    Compares True Positive Rate (TPR) across groups defined by `sensitive_attribute_col_values`.\n",
    "    TPR = P(predicted_label = 1 | true_label = 1, group).\n",
    "    A model satisfies Equal Opportunity if P(ŷ=1 | Y=1, A=a) is the same for all groups 'a'.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (pd.Series): Ground truth labels (0 or 1).\n",
    "        y_pred_values (np.ndarray): Predicted labels (0 or 1) by the model.\n",
    "        sensitive_attribute_col_values (pd.Series): Values of the sensitive attribute (0 or 1)\n",
    "                                                    corresponding to y_true and y_pred_values.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float]: \n",
    "            tpr_group1 (float): True Positive Rate for the group where sensitive_attribute_value == 1.\n",
    "            tpr_group0 (float): True Positive Rate for the group where sensitive_attribute_value == 0.\n",
    "            abs_diff (float): Absolute difference between tpr_group1 and tpr_group0.\n",
    "                                A smaller difference indicates better fairness in terms of equal opportunity.\n",
    "                                Returns np.nan for a group's TPR if there are no true positive instances in that group.\n",
    "    \"\"\"\n",
    "    # Mask for instances where the true label is positive (Y=1)\n",
    "    true_positives_mask = (y_true == 1)\n",
    "\n",
    "    def _calculate_tpr_for_subgroup(subgroup_identity_mask: pd.Series) -> float:\n",
    "        \"\"\"Helper to calculate TPR for a specific subgroup among true positives.\"\"\"\n",
    "        # Combine true positive mask with the mask identifying the subgroup\n",
    "        combined_mask = true_positives_mask & subgroup_identity_mask\n",
    "        # Calculate TPR if any such instances exist, otherwise return NaN\n",
    "        # This is effectively sum(predictions_for_group_among_true_positives) / count(group_among_true_positives).\n",
    "        return (y_pred_values[combined_mask].mean() if combined_mask.any() else np.nan)\n",
    "\n",
    "    # TPR for the group where sensitive attribute is 1 (female)\n",
    "    tpr_group1 = _calculate_tpr_for_subgroup(sensitive_attribute_col_values == 1)\n",
    "    # TPR for the group where sensitive attribute is 0 (male)\n",
    "    tpr_group0 = _calculate_tpr_for_subgroup(sensitive_attribute_col_values == 0)\n",
    "    \n",
    "    # Absolute difference in TPRs\n",
    "    abs_tpr_difference = abs(tpr_group1 - tpr_group0) if pd.notna(tpr_group1) and pd.notna(tpr_group0) else np.nan\n",
    "    \n",
    "    return tpr_group1, tpr_group0, abs_tpr_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a900b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Fairness Analysis Function ---\n",
    "\n",
    "def run_fairness_analysis_scenario(\n",
    "    x_fit: pd.DataFrame,\n",
    "    y_fit: pd.Series,\n",
    "    x_eval_train: pd.DataFrame,\n",
    "    y_eval_train: pd.Series,\n",
    "    x_eval_test: pd.DataFrame,\n",
    "    y_eval_test: pd.Series,\n",
    "    sens_attr_name: str,\n",
    "    model_hyperparams: dict,\n",
    "    include_sens_attr_in_training: bool,\n",
    "    description_suffix: str\n",
    ") -> tuple[RandomForestClassifier, dict]:\n",
    "    \"\"\"\n",
    "    Runs a complete fairness analysis scenario for a model.\n",
    "    This includes:\n",
    "    1. Preparing features for training (optionally including/excluding the sensitive attribute).\n",
    "    2. Training a RandomForestClassifier.\n",
    "    3. Making predictions on evaluation datasets (training and test sets, using original distributions).\n",
    "    4. Calculating and printing Equal Opportunity fairness metrics.\n",
    "\n",
    "    Parameters:\n",
    "        x_fit (pd.DataFrame): Resampled training features for model fitting.\n",
    "        y_fit (pd.Series): Resampled training labels for model fitting.\n",
    "        x_eval_train (pd.DataFrame): Preprocessed training features (original distribution) for evaluation.\n",
    "        y_eval_train (pd.Series): Preprocessed training labels (original distribution) for evaluation.\n",
    "        x_eval_test (pd.DataFrame): Preprocessed test features for evaluation.\n",
    "        y_eval_test (pd.Series): Preprocessed test labels for evaluation.\n",
    "        sens_attr_name (str): The name of the sensitive attribute column.\n",
    "        model_hyperparams (dict): Hyperparameters for the RandomForestClassifier.\n",
    "        include_sens_attr_in_training (bool): If True, the sensitive attribute is included in training features.\n",
    "                                              If False, it's excluded.\n",
    "        description_suffix (str): A string to append to print statements (e.g., \"WITH Gender Column\").\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            trained_rf_model (RandomForestClassifier): The trained model instance.\n",
    "            fairness_metrics_results (dict): A dictionary containing fairness metrics for train and test sets.\n",
    "                                             Example: {'train': (p1, p2, diff), 'test': (p1, p2, diff)}\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Fairness Scenario: Model {description_suffix} ---\")\n",
    "\n",
    "    # 1. Prepare features for fitting the model.\n",
    "    #    `drop_sensitive_attr` is the inverse of `include_sens_attr_in_training`.\n",
    "    #    A copy of x_fit is used to avoid modifying the original DataFrame.\n",
    "    print(f\"Preparing features for fitting model {description_suffix} (from resampled data)...\")\n",
    "    features_for_fitting = _prepare_features(\n",
    "        df=x_fit.copy(),\n",
    "        sensitive_attr_column_name=sens_attr_name,\n",
    "        drop_sensitive_attr=(not include_sens_attr_in_training) # If including, don't drop. If not including, do drop.\n",
    "    )\n",
    "    \n",
    "    # 2. Initialize and train the Random Forest classifier.\n",
    "    trained_rf_model = RandomForestClassifier(**model_hyperparams)\n",
    "    print(f\"Training Random Forest model {description_suffix} (on resampled data)...\")\n",
    "    trained_rf_model.fit(features_for_fitting, y_fit)\n",
    "\n",
    "    # 3. Make predictions on the EVALUATION datasets (original preprocessed distributions).\n",
    "    #    Features for prediction must be prepared consistently with how the model was trained.\n",
    "    \n",
    "    # Predictions for training set fairness evaluation:\n",
    "    # Use x_eval_train (original distribution).\n",
    "    features_for_train_eval_pred = _prepare_features(\n",
    "        df=x_eval_train.copy(),\n",
    "        sensitive_attr_column_name=sens_attr_name,\n",
    "        drop_sensitive_attr=(not include_sens_attr_in_training)\n",
    "    )\n",
    "    y_pred_train_eval_values = trained_rf_model.predict(features_for_train_eval_pred)\n",
    "\n",
    "    # Predictions for test set performance and fairness evaluation:\n",
    "    # Use x_eval_test (original distribution).\n",
    "    features_for_test_eval_pred = _prepare_features(\n",
    "        df=x_eval_test.copy(),\n",
    "        sensitive_attr_column_name=sens_attr_name,\n",
    "        drop_sensitive_attr=(not include_sens_attr_in_training)\n",
    "    )\n",
    "    y_pred_test_eval_values = trained_rf_model.predict(features_for_test_eval_pred)\n",
    "\n",
    "    # 4. Calculate Equal Opportunity metrics.\n",
    "    #    On the training set (using original distribution data for evaluation).\n",
    "    eo_p1_train, eo_p0_train, eo_diff_train = calculate_equal_opportunity(\n",
    "        y_true=y_eval_train,\n",
    "        y_pred_values=y_pred_train_eval_values,\n",
    "        sensitive_attribute_col_values=x_eval_train[sens_attr_name]\n",
    "    )\n",
    "    #    On the test set.\n",
    "    eo_p1_test, eo_p0_test, eo_diff_test = calculate_equal_opportunity(\n",
    "        y_true=y_eval_test,\n",
    "        y_pred_values=y_pred_test_eval_values,\n",
    "        sensitive_attribute_col_values=x_eval_test[sens_attr_name]\n",
    "    )\n",
    "\n",
    "    # Store fairness metrics\n",
    "    fairness_metrics_results = {\n",
    "        'train': {'tpr_group1': eo_p1_train, 'tpr_group0': eo_p0_train, 'diff': eo_diff_train},\n",
    "        'test':  {'tpr_group1': eo_p1_test,  'tpr_group0': eo_p0_test,  'diff': eo_diff_test}\n",
    "    }\n",
    "    \n",
    "    # Print the fairness results. Assuming group 1 is female, group 0 is male based on SENSITIVE_ATTR_NAME context.\n",
    "    print(f\"\\n=== Fairness Metrics: Model {description_suffix} ===\")\n",
    "    print(f\"Equal Opportunity - Training Set (Original Dist.): Female TPR={eo_p1_train:.3f}, Male TPR={eo_p0_train:.3f}, Difference={eo_diff_train:.3f}\")\n",
    "    print(f\"Equal Opportunity - Test Set                     : Female TPR={eo_p1_test:.3f}, Male TPR={eo_p0_test:.3f}, Difference={eo_diff_test:.3f}\")\n",
    "    \n",
    "    return trained_rf_model, fairness_metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf0a7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Performance Reporting Function (remains the same, called after scenario function) ---\n",
    "def show_performance_report(\n",
    "    estimator: RandomForestClassifier, \n",
    "    x_data_for_eval: pd.DataFrame, \n",
    "    y_data_for_eval: pd.Series,    \n",
    "    model_description_text: str, \n",
    "    drop_sens_attr_for_predict: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Prints a classification report and balanced accuracy for a given model on specified evaluation data.\n",
    "    \"\"\"\n",
    "    # Prepare features for prediction, ensuring consistency with how the model was trained.\n",
    "    features_for_prediction = _prepare_features(\n",
    "        df=x_data_for_eval.copy(), \n",
    "        sensitive_attr_column_name=SENSITIVE_ATTR_NAME, # Uses global SENSITIVE_ATTR_NAME\n",
    "        drop_sensitive_attr=drop_sens_attr_for_predict\n",
    "    )\n",
    "    y_model_predictions = estimator.predict(features_for_prediction)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\"--- Performance Report: {model_description_text} ---\")\n",
    "    print(\"-\"*70)\n",
    "    print(classification_report(y_data_for_eval, y_model_predictions, zero_division=0, digits=4))\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_score(y_data_for_eval, y_model_predictions):.4f}\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a21d6eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Fairness Scenario: Model WITH GENDER Column ---\n",
      "Preparing features for fitting model WITH GENDER Column (from resampled data)...\n",
      "Training Random Forest model WITH GENDER Column (on resampled data)...\n",
      "\n",
      "=== Fairness Metrics: Model WITH GENDER Column ===\n",
      "Equal Opportunity - Training Set (Original Dist.): Female TPR=0.871, Male TPR=0.939, Difference=0.067\n",
      "Equal Opportunity - Test Set                     : Female TPR=0.759, Male TPR=0.870, Difference=0.112\n",
      "\n",
      "--- Running Fairness Scenario: Model WITHOUT GENDER Column ---\n",
      "Preparing features for fitting model WITHOUT GENDER Column (from resampled data)...\n",
      "Training Random Forest model WITHOUT GENDER Column (on resampled data)...\n",
      "\n",
      "=== Fairness Metrics: Model WITHOUT GENDER Column ===\n",
      "Equal Opportunity - Training Set (Original Dist.): Female TPR=0.889, Male TPR=0.925, Difference=0.036\n",
      "Equal Opportunity - Test Set                     : Female TPR=0.808, Male TPR=0.816, Difference=0.008\n"
     ]
    }
   ],
   "source": [
    "# --- Execute Fairness Analysis Scenarios ---\n",
    "\n",
    "# Scenario 1: Model trained WITH the sensitive attribute ('GENDER')\n",
    "rf_model_with_gender, fairness_results_with_gender = run_fairness_analysis_scenario(\n",
    "    x_fit=X_train_fit,\n",
    "    y_fit=y_train_fit,\n",
    "    x_eval_train=X_train_eval,\n",
    "    y_eval_train=y_train_eval,\n",
    "    x_eval_test=X_test_eval,\n",
    "    y_eval_test=y_test_eval,\n",
    "    sens_attr_name=SENSITIVE_ATTR_NAME,\n",
    "    model_hyperparams=BEST_RF_MODEL_PARAMS,\n",
    "    include_sens_attr_in_training=True, # Key flag for this scenario\n",
    "    description_suffix=\"WITH GENDER Column\"\n",
    ")\n",
    "\n",
    "# Scenario 2: Model trained WITHOUT the sensitive attribute ('GENDER')\n",
    "rf_model_without_gender, fairness_results_without_gender = run_fairness_analysis_scenario(\n",
    "    x_fit=X_train_fit,\n",
    "    y_fit=y_train_fit,\n",
    "    x_eval_train=X_train_eval,\n",
    "    y_eval_train=y_train_eval,\n",
    "    x_eval_test=X_test_eval,\n",
    "    y_eval_test=y_test_eval,\n",
    "    sens_attr_name=SENSITIVE_ATTR_NAME,\n",
    "    model_hyperparams=BEST_RF_MODEL_PARAMS,\n",
    "    include_sens_attr_in_training=False, # Key flag for this scenario\n",
    "    description_suffix=\"WITHOUT GENDER Column\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c06ea5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "--- Performance Report: Random Forest WITH GENDER – Test Set ---\n",
      "----------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9700    0.4781    0.6405      9341\n",
      "           1     0.1144    0.8203    0.2009       768\n",
      "\n",
      "    accuracy                         0.5041     10109\n",
      "   macro avg     0.5422    0.6492    0.4207     10109\n",
      "weighted avg     0.9050    0.5041    0.6071     10109\n",
      "\n",
      "Balanced Accuracy: 0.6492\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "--- Performance Report: Random Forest WITHOUT GENDER – Test Set ---\n",
      "----------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9676    0.4602    0.6238      9341\n",
      "           1     0.1101    0.8125    0.1940       768\n",
      "\n",
      "    accuracy                         0.4870     10109\n",
      "   macro avg     0.5389    0.6364    0.4089     10109\n",
      "weighted avg     0.9024    0.4870    0.5911     10109\n",
      "\n",
      "Balanced Accuracy: 0.6364\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Display Performance Reports for Both Models on the TEST SET ---\n",
    "\n",
    "# Performance for the model trained WITH Gender\n",
    "show_performance_report(\n",
    "    estimator=rf_model_with_gender, \n",
    "    x_data_for_eval=X_test_eval, \n",
    "    y_data_for_eval=y_test_eval, \n",
    "    model_description_text=\"Random Forest WITH GENDER – Test Set\", \n",
    "    drop_sens_attr_for_predict=False # Model was trained with gender, so don't drop for prediction\n",
    ")\n",
    "\n",
    "# Performance for the model trained WITHOUT Gender\n",
    "show_performance_report(\n",
    "    estimator=rf_model_without_gender,  \n",
    "    x_data_for_eval=X_test_eval, \n",
    "    y_data_for_eval=y_test_eval, \n",
    "    model_description_text=\"Random Forest WITHOUT GENDER – Test Set\", \n",
    "    drop_sens_attr_for_predict=True # Model was trained without gender, so drop for prediction\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
