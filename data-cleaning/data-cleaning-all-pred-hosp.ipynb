{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data\n",
    "\n",
    "# SINAN DataSUS CSV files path (modify to match your file path)\n",
    "sinan_path = os.path.expanduser('~/Desktop/DataSUS-Chikungunya-ML/source/csv/')\n",
    "\n",
    "# Cleaned CSV files path (modify to match your file path)\n",
    "cleaned_path = os.path.expanduser('~/Desktop/DataSUS-Chikungunya-ML/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns common to all files:\n",
      "{'ALRM_ABDOM', 'GRAV_MELEN', 'ARTRALGIA', 'DT_ENCERRA', 'ID_RG_RESI', 'ALRM_HEMAT', 'ARTRITE', 'FEBRE', 'SEM_PRI', 'UF', 'CRITERIO', 'ID_MUNICIP', 'CS_GESTANT', 'VOMITO', 'GENGIVO', 'DOR_COSTAS', 'RES_CHIKS1', 'MIALGIA', 'NU_IDADE_N', 'TPAUTOCTO', 'RESUL_SORO', 'ID_REGIONA', 'SG_UF_NOT', 'ALRM_LIQ', 'NDUPLIC_N', 'CEFALEIA', 'MUNICIPIO', 'CON_FHD', 'RESUL_PCR_', 'SG_UF', 'RES_CHIKS2', 'COMPLICA', 'RENAL', 'DIABETES', 'GRAV_MIOC', 'CS_RACA', 'COMUNINF', 'ALRM_LETAR', 'GRAV_ENCH', 'GRAV_HIPOT', 'ALRM_VOM', 'DT_CHIK_S1', 'LACO_N', 'GRAV_HEMAT', 'HEMATURA', 'HEPATOPAT', 'DT_CHIK_S2', 'DT_NS1', 'GRAV_SANG', 'LACO', 'DT_INTERNA', 'CLASSI_FIN', 'AUTO_IMUNE', 'ID_MN_RESI', 'DT_ALRM', 'DT_PRNT', 'GRAV_INSUF', 'HIPERTENSA', 'DT_NOTIFIC', 'NU_ANO', 'ID_OCUPA_N', 'DT_PCR', 'COPAISINF', 'DT_GRAV', 'GRAV_ORGAO', 'SOROTIPO', 'IMUNOH_N', 'HOSPITALIZ', 'DT_VIRAL', 'PLASMATICO', 'EXANTEMA', 'GRAV_METRO', 'ALRM_SANG', 'DOR_RETRO', 'RESUL_VI_N', 'MANI_HEMOR', 'HISTOPA_N', 'DOENCA_TRA', 'GRAV_AST', 'COUFINF', 'EPISTAXE', 'RESUL_PRNT', 'PETEQUIA_N', 'CLINC_CHIK', 'CS_ESCOL_N', 'ALRM_PLAQ', 'EVOLUCAO', 'CONJUNTVIT', 'ALRM_HEPAT', 'ACIDO_PEPT', 'TP_NOT', 'RESUL_NS1', 'EVIDENCIA', 'ID_AGRAVO', 'GRAV_CONSC', 'TP_SISTEMA', 'SANGRAM', 'ALRM_HIPOT', 'LEUCOPENIA', 'GRAV_TAQUI', 'GRAV_PULSO', 'DT_INVEST', 'NAUSEA', 'DT_SIN_PRI', 'PETEQUIAS', 'GRAV_EXTRE', 'HEMATOLOG', 'GRAV_CONV', 'ID_PAIS', 'SEM_NOT', 'DT_OBITO', 'METRO', 'PLAQ_MENOR', 'CS_SEXO', 'ID_UNIDADE', 'DT_SORO', 'Unnamed: 0'}\n",
      "\n",
      "Columns that are not common among all files:\n",
      "Column 'ANO_NASC' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'FLXRECEBI' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'NU_LOTE_I' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'CS_FLXRET' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'MIGRADO_W' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'DT_DIGITA' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the Chikungunya data from the CSV files\n",
    "The CSV files are named CHIKBRYY.csv, where YY is the last two digits of the year\n",
    "The files are stored in the ~/Downloads/dbc2csv/source/csv/ directory\n",
    "The columns in the CSV files are not consistent across all years\n",
    "We want to identify the columns that are common to all files\n",
    "\"\"\"\n",
    "\n",
    "# List of last two digits of years for which we have CSV files (2018 to 2024)\n",
    "start_year = 18\n",
    "end_year = 25\n",
    "\n",
    "# Number of years to use for testing\n",
    "test_years = 2\n",
    "\n",
    "assert start_year < end_year, \"Start year must be less than end year\"\n",
    "years = range(start_year, end_year + 1)\n",
    "\n",
    "assert len(years) > test_years, \"At least {test_years + 1}  years of data are required.\"\n",
    "\n",
    "# Dictionary to store the columns for each file\n",
    "file_columns = {}\n",
    "\n",
    "# Loop through each year, build the filename, and read the CSV\n",
    "for year in years:\n",
    "    file_name = f'{sinan_path}CHIKBR{str(year)}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_name, low_memory=False)\n",
    "        # Save the set of columns for this file\n",
    "        file_columns[file_name] = set(df.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "\n",
    "# Ensure we have loaded at least one file before proceeding\n",
    "assert file_columns, \"No files were loaded. Please check your file paths.\"\n",
    "\n",
    "# Find common columns: the intersection of columns across all files\n",
    "common_columns = set.intersection(*file_columns.values())\n",
    "assert common_columns, \"No common columns found. Please check your file paths.\"\n",
    "\n",
    "print(\"\\nColumns common to all files:\")\n",
    "print(common_columns)\n",
    "\n",
    "# Compute the union of all columns (all columns that appear in any file)\n",
    "all_columns = set.union(*file_columns.values())\n",
    "\n",
    "# For columns that are not common, print which files have them and which don't.\n",
    "print(\"\\nColumns that are not common among all files:\")\n",
    "for col in all_columns - common_columns:\n",
    "    # Extract base name (e.g., CHIKBR21) from each file path\n",
    "    files_with = [os.path.splitext(os.path.basename(fname))[0] \n",
    "                    for fname, cols in file_columns.items() if col in cols]\n",
    "    files_without = [os.path.splitext(os.path.basename(fname))[0] \n",
    "                        for fname, cols in file_columns.items() if col not in cols]\n",
    "    print(f\"Column '{col}' is present in files: {files_with} and missing in files: {files_without}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the concatenated X_train DataFrame:  (1050480, 116)\n",
      "Shape of the concatenated X_test DataFrame:  (449729, 116)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove columns that are not common to all files\n",
    "Create a new DataFrame with only the common columns\n",
    "Concatenate all the DataFrames without the last {test_years} years into a single DataFrame called X_train\n",
    "Concatenate the DataFrames from the last {test_years} years into a single DataFrame called X_test\n",
    "\"\"\"\n",
    "\n",
    "# Load the train and test data for each year, keeping only the common columns\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for year in years:\n",
    "    file_name = f'{sinan_path}CHIKBR{str(year)}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_name, usecols=common_columns, low_memory=False)\n",
    "        # Drop the \"Unnamed: 0\" column if present\n",
    "        if \"Unnamed: 0\" in df.columns:\n",
    "            df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "        if year < years[-test_years]:\n",
    "            X_train.append(df)\n",
    "        else:\n",
    "            X_test.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "\n",
    "# Concatenate all the DataFrames into a single train and test DataFrame\n",
    "X_train = pd.concat(X_train, ignore_index=True)\n",
    "X_test = pd.concat(X_test, ignore_index=True)\n",
    "\n",
    "# Display the shape of the concatenated DataFrame\n",
    "print(\"\\nShape of the concatenated X_train DataFrame: \", X_train.shape)\n",
    "print(\"Shape of the concatenated X_test DataFrame: \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in the 'HOSPITALIZ' column:\n",
      "HOSPITALIZ\n",
      "2.0    537813\n",
      "2       73033\n",
      "1.0     28859\n",
      "9.0     22044\n",
      "1        3629\n",
      "9        2885\n",
      "           1\n",
      "Ø           1\n",
      "J           1\n",
      "ï           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Shape of the filtered X_train DataFrame:  (566672, 116)\n",
      "Shape of the filtered X_test DataFrame:  (311808, 116)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Filter the data to include only the rows where the pacient was hospitalized (\"HOSPITALIZ\" column is equal to 1 or to 1.0)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nUnique values in the 'HOSPITALIZ' column:\")\n",
    "print(X_train['HOSPITALIZ'].value_counts())\n",
    "\n",
    "# Filter the data to include only the rows where the patient was either hospitalized or not (remove ignored values)\n",
    "X_train = X_train[(X_train['HOSPITALIZ'] == 1) | (X_train['HOSPITALIZ'] == 1.0) | (X_train['HOSPITALIZ'] == 2) | (X_train['HOSPITALIZ'] == 2.0)]\n",
    "X_test = X_test[(X_test['HOSPITALIZ'] == 1) | (X_test['HOSPITALIZ'] == 1.0) | (X_test['HOSPITALIZ'] == 2) | (X_test['HOSPITALIZ'] == 2.0)]\n",
    "\n",
    "# Display the shape of the filtered DataFrame\n",
    "print(\"\\nShape of the filtered X_train DataFrame: \", X_train.shape)\n",
    "print(\"Shape of the filtered X_test DataFrame: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values for 'HOSPITALIZ' in y_train:\n",
      "HOSPITALIZ\n",
      "0    537813\n",
      "1     28859\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Values for 'HOSPITALIZ' in y_test:\n",
      "HOSPITALIZ\n",
      "0    297865\n",
      "1     13943\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'HOSPITALIZ' column from X_train and X_test and use it as the target label\n",
    "y_train = X_train.pop(\"HOSPITALIZ\")\n",
    "y_test = X_test.pop(\"HOSPITALIZ\")\n",
    "\n",
    "# Map the target: 1 (positive) becomes 1, 2 (negative) becomes 0\n",
    "y_train = y_train.map({1: 1, 2: 0})\n",
    "y_test = y_test.map({1: 1, 2: 0})\n",
    "\n",
    "# Print the value counts for the target variable\n",
    "print(\"\\nValues for 'HOSPITALIZ' in y_train:\")\n",
    "print(y_train.value_counts(dropna=True))\n",
    "\n",
    "print(\"\\nValues for 'HOSPITALIZ' in y_test:\")\n",
    "print(y_test.value_counts(dropna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped constant columns (constant values): ['TP_NOT', 'ID_AGRAVO', 'ID_PAIS', 'SOROTIPO', 'HISTOPA_N', 'TP_SISTEMA', 'NDUPLIC_N']\n"
     ]
    }
   ],
   "source": [
    "# Remove columns where all values are the same in X_train and X_test (constant columns)\n",
    "constant_columns = [col for col in X_train.columns if X_train[col].nunique() == 1]\n",
    "\n",
    "# Drop these constant columns from both X_train and X_test\n",
    "X_train = X_train.drop(columns=constant_columns)\n",
    "X_test = X_test.drop(columns=constant_columns, errors='ignore')\n",
    "\n",
    "print(\"\\nDropped constant columns (constant values):\", constant_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of missing values in X_train:\n",
      "DT_NOTIFIC    0.0\n",
      "SEM_NOT       0.0\n",
      "NU_ANO        0.0\n",
      "SG_UF_NOT     0.0\n",
      "ID_MUNICIP    0.0\n",
      "             ... \n",
      "PLASMATICO    1.0\n",
      "EVIDENCIA     1.0\n",
      "PLAQ_MENOR    1.0\n",
      "CON_FHD       1.0\n",
      "COMPLICA      1.0\n",
      "Length: 108, dtype: float64\n",
      "\n",
      "Percentage of missing values in X_test:\n",
      "DT_NOTIFIC    0.0\n",
      "SEM_NOT       0.0\n",
      "NU_ANO        0.0\n",
      "SG_UF_NOT     0.0\n",
      "ID_MUNICIP    0.0\n",
      "             ... \n",
      "PLASMATICO    1.0\n",
      "EVIDENCIA     1.0\n",
      "PLAQ_MENOR    1.0\n",
      "CON_FHD       1.0\n",
      "COMPLICA      1.0\n",
      "Length: 108, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Identify and remove columns with more than missing_values_threshold missing values in X_train and X_test\n",
    "\"\"\"\n",
    "\n",
    "missing_values_threshold = 0.20\n",
    "assert 0.0 <= missing_values_threshold <= 1.0, \"missing_values_threshold must be between 0 and 1\"\n",
    "\n",
    "# Compute the percentage of missing values in each column of X_train\n",
    "missing_values_train = X_train.isnull().mean()\n",
    "missing_values_test = X_test.isnull().mean()\n",
    "missing_values_mean = (missing_values_train + missing_values_test) / 2\n",
    "\n",
    "# Print the percentage of missing values in each column of X_train\n",
    "print(\"\\nPercentage of missing values in X_train:\")\n",
    "print(missing_values_train)\n",
    "\n",
    "# Print the percentage of missing values in each column of X_test\n",
    "print(\"\\nPercentage of missing values in X_test:\")\n",
    "print(missing_values_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped columns (more than 20.0% missing):\n",
      "['ID_OCUPA_N', 'DT_CHIK_S1', 'DT_CHIK_S2', 'DT_PRNT', 'RES_CHIKS1', 'RES_CHIKS2', 'RESUL_PRNT', 'DT_SORO', 'RESUL_SORO', 'DT_NS1', 'RESUL_NS1', 'DT_VIRAL', 'RESUL_VI_N', 'DT_PCR', 'RESUL_PCR_', 'IMUNOH_N', 'DT_INTERNA', 'UF', 'MUNICIPIO', 'TPAUTOCTO', 'COUFINF', 'COPAISINF', 'COMUNINF', 'DOENCA_TRA', 'CLINC_CHIK', 'DT_OBITO', 'ALRM_HIPOT', 'ALRM_PLAQ', 'ALRM_VOM', 'ALRM_SANG', 'ALRM_HEMAT', 'ALRM_ABDOM', 'ALRM_LETAR', 'ALRM_HEPAT', 'ALRM_LIQ', 'DT_ALRM', 'GRAV_PULSO', 'GRAV_CONV', 'GRAV_ENCH', 'GRAV_INSUF', 'GRAV_TAQUI', 'GRAV_EXTRE', 'GRAV_HIPOT', 'GRAV_HEMAT', 'GRAV_MELEN', 'GRAV_METRO', 'GRAV_SANG', 'GRAV_AST', 'GRAV_MIOC', 'GRAV_CONSC', 'GRAV_ORGAO', 'DT_GRAV', 'MANI_HEMOR', 'EPISTAXE', 'GENGIVO', 'METRO', 'PETEQUIAS', 'HEMATURA', 'SANGRAM', 'LACO_N', 'PLASMATICO', 'EVIDENCIA', 'PLAQ_MENOR', 'CON_FHD', 'COMPLICA']\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = missing_values_train[missing_values_mean > missing_values_threshold].index\n",
    "\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"\\nDropped columns (more than {missing_values_threshold * 100}% missing):\")\n",
    "print(list(cols_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the filtered X_train DataFrame:  (566672, 43)\n",
      "Shape of the filtered X_test DataFrame:  (311808, 43)\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the filtered DataFrame\n",
    "print(\"\\nShape of the filtered X_train DataFrame: \", X_train.shape)\n",
    "print(\"Shape of the filtered X_test DataFrame: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the filtered data to a new CSV file in the cleaned_path directory\n",
    "\"\"\"\n",
    "\n",
    "# Save the filtered data to a new CSV file in the cleaned_path directory\n",
    "X_train.to_csv(f'{cleaned_path}X_train.csv', index=False)\n",
    "y_train.to_csv(f'{cleaned_path}y_train.csv', index=False)\n",
    "\n",
    "X_test.to_csv(f'{cleaned_path}X_test.csv', index=False)\n",
    "y_test.to_csv(f'{cleaned_path}y_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
