{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data\n",
    "\n",
    "# SINAN DataSUS CSV files path (modify to match your file path)\n",
    "sinan_path = os.path.expanduser('~/Downloads/dbc2csv/source/csv/')\n",
    "\n",
    "# Cleaned CSV files path (modify to match your file path)\n",
    "cleaned_path = os.path.expanduser('~/Downloads/dbc2csv/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns common to all files:\n",
      "{'CS_ESCOL_N', 'LACO_N', 'ID_AGRAVO', 'GRAV_ENCH', 'GRAV_SANG', 'COMUNINF', 'ID_RG_RESI', 'MUNICIPIO', 'ALRM_PLAQ', 'RES_CHIKS2', 'HOSPITALIZ', 'ALRM_LIQ', 'PLAQ_MENOR', 'ID_OCUPA_N', 'CS_RACA', 'TPAUTOCTO', 'COPAISINF', 'DT_GRAV', 'SG_UF', 'LEUCOPENIA', 'GRAV_HEMAT', 'PETEQUIA_N', 'SG_UF_NOT', 'ARTRALGIA', 'MANI_HEMOR', 'RES_CHIKS1', 'DOR_RETRO', 'GRAV_EXTRE', 'ALRM_SANG', 'EPISTAXE', 'ALRM_VOM', 'IMUNOH_N', 'Unnamed: 0', 'RENAL', 'CRITERIO', 'DT_SIN_PRI', 'HEPATOPAT', 'ACIDO_PEPT', 'EVIDENCIA', 'DOR_COSTAS', 'PLASMATICO', 'CON_FHD', 'DT_PCR', 'HEMATURA', 'HIPERTENSA', 'COUFINF', 'NU_ANO', 'CONJUNTVIT', 'GRAV_CONSC', 'GRAV_MIOC', 'GRAV_INSUF', 'RESUL_SORO', 'ALRM_HEPAT', 'EVOLUCAO', 'DT_NS1', 'DOENCA_TRA', 'GENGIVO', 'ALRM_HIPOT', 'RESUL_PCR_', 'ALRM_ABDOM', 'CLASSI_FIN', 'GRAV_TAQUI', 'EXANTEMA', 'GRAV_CONV', 'DT_VIRAL', 'AUTO_IMUNE', 'SOROTIPO', 'MIALGIA', 'FEBRE', 'DT_CHIK_S1', 'CLINC_CHIK', 'RESUL_NS1', 'ID_MUNICIP', 'NU_IDADE_N', 'RESUL_VI_N', 'ID_UNIDADE', 'ID_PAIS', 'SEM_PRI', 'RESUL_PRNT', 'GRAV_AST', 'GRAV_HIPOT', 'METRO', 'PETEQUIAS', 'LACO', 'DT_PRNT', 'GRAV_MELEN', 'GRAV_METRO', 'DT_OBITO', 'DT_SORO', 'HEMATOLOG', 'ARTRITE', 'DT_INVEST', 'ALRM_LETAR', 'GRAV_ORGAO', 'SANGRAM', 'ALRM_HEMAT', 'CEFALEIA', 'NDUPLIC_N', 'DT_INTERNA', 'UF', 'NAUSEA', 'COMPLICA', 'ID_MN_RESI', 'DT_CHIK_S2', 'GRAV_PULSO', 'HISTOPA_N', 'TP_NOT', 'ID_REGIONA', 'DIABETES', 'VOMITO', 'DT_ENCERRA', 'CS_GESTANT', 'DT_ALRM', 'TP_SISTEMA', 'SEM_NOT', 'DT_NOTIFIC', 'CS_SEXO'}\n",
      "\n",
      "Columns that are not common among all files:\n",
      "Column 'MIGRADO_W' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'CS_FLXRET' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'NU_LOTE_I' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'FLXRECEBI' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'DT_DIGITA' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'ANO_NASC' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the Chikungunya data from the CSV files\n",
    "The CSV files are named CHIKBRYY.csv, where YY is the last two digits of the year\n",
    "The files are stored in the ~/Downloads/dbc2csv/source/csv/ directory\n",
    "The columns in the CSV files are not consistent across all years\n",
    "We want to identify the columns that are common to all files\n",
    "\"\"\"\n",
    "\n",
    "# List of last two digits of years for which we have CSV files (2018 to 2024)\n",
    "start_year = 18\n",
    "end_year = 24\n",
    "assert start_year < end_year, \"Start year must be less than end year\"\n",
    "years = range(start_year, end_year + 1)\n",
    "\n",
    "# Dictionary to store the columns for each file\n",
    "file_columns = {}\n",
    "\n",
    "# Loop through each year, build the filename, and read the CSV\n",
    "for year in years:\n",
    "    file_name = f'{sinan_path}CHIKBR{str(year)}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_name, low_memory=False)\n",
    "        # Save the set of columns for this file\n",
    "        file_columns[file_name] = set(df.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "\n",
    "# Ensure we have loaded at least one file before proceeding\n",
    "assert file_columns, \"No files were loaded. Please check your file paths.\"\n",
    "\n",
    "# Find common columns: the intersection of columns across all files\n",
    "common_columns = set.intersection(*file_columns.values())\n",
    "print(\"\\nColumns common to all files:\")\n",
    "print(common_columns)\n",
    "\n",
    "# Compute the union of all columns (all columns that appear in any file)\n",
    "all_columns = set.union(*file_columns.values())\n",
    "\n",
    "# For columns that are not common, print which files have them and which don't.\n",
    "print(\"\\nColumns that are not common among all files:\")\n",
    "for col in all_columns - common_columns:\n",
    "    # Extract base name (e.g., CHIKBR21) from each file path\n",
    "    files_with = [os.path.splitext(os.path.basename(fname))[0] \n",
    "                    for fname, cols in file_columns.items() if col in cols]\n",
    "    files_without = [os.path.splitext(os.path.basename(fname))[0] \n",
    "                        for fname, cols in file_columns.items() if col not in cols]\n",
    "    print(f\"Column '{col}' is present in files: {files_with} and missing in files: {files_without}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the concatenated X_train DataFrame:  (802287, 117)\n",
      "Shape of the concatenated X_test DataFrame:  (650214, 117)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove columns that are not common to all files\n",
    "Create a new DataFrame with only the common columns\n",
    "Concatenate all the DataFrames without the last {test_years} years into a single DataFrame called X_train\n",
    "Concatenate the DataFrames from the last {test_years} years into a single DataFrame called X_test\n",
    "\"\"\"\n",
    "\n",
    "# Number of years to use for testing\n",
    "test_years = 2\n",
    "assert common_columns, \"No common columns found. Please check your file paths.\"\n",
    "assert len(years) > test_years, \"At least {test_years + 1}  years of data are required.\"\n",
    "\n",
    "# Load the train and test data for each year, keeping only the common columns\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for year in years:\n",
    "    file_name = f'{sinan_path}CHIKBR{str(year)}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_name, usecols=common_columns, low_memory=False)\n",
    "        if year < years[-test_years]:\n",
    "            X_train.append(df)\n",
    "        else:\n",
    "            X_test.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "\n",
    "# Concatenate all the DataFrames into a single train and test DataFrame\n",
    "X_train = pd.concat(X_train, ignore_index=True)\n",
    "X_test = pd.concat(X_test, ignore_index=True)\n",
    "\n",
    "# Display the shape of the concatenated DataFrame\n",
    "print(\"\\nShape of the concatenated X_train DataFrame: \", X_train.shape)\n",
    "print(\"Shape of the concatenated X_test DataFrame: \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in the 'HOSPITALIZ' column:\n",
      "HOSPITALIZ\n",
      "2.0    393418\n",
      "2       73033\n",
      "1.0     21844\n",
      "9.0     15658\n",
      "1        3629\n",
      "9        2885\n",
      "           1\n",
      "Ø           1\n",
      "J           1\n",
      "ï           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Shape of the filtered X_train DataFrame:  (21844, 117)\n",
      "Shape of the filtered X_test DataFrame:  (18939, 117)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Filter the data to include only the rows where the pacient was hospitalized (\"HOSPITALIZ\" column is equal to 1 or to 1.0)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nUnique values in the 'HOSPITALIZ' column:\")\n",
    "print(X_train['HOSPITALIZ'].value_counts())\n",
    "\n",
    "# Filter the data to include only the rows where the patient was hospitalized\n",
    "X_train = X_train[(X_train['HOSPITALIZ'] == 1) | (X_train['HOSPITALIZ'] == 1.0)]\n",
    "X_test = X_test[(X_test['HOSPITALIZ'] == 1) | (X_test['HOSPITALIZ'] == 1.0)]\n",
    "\n",
    "# Display the shape of the filtered DataFrame\n",
    "print(\"\\nShape of the filtered X_train DataFrame: \", X_train.shape)\n",
    "print(\"Shape of the filtered X_test DataFrame: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values for 'EVOLUCAO' in X_train:\n",
      "EVOLUCAO\n",
      "1.0    15203\n",
      "NaN     3865\n",
      "9.0     1587\n",
      "3.0      858\n",
      "2.0      251\n",
      "4.0       80\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Values for 'EVOLUCAO' in X_test:\n",
      "EVOLUCAO\n",
      "1.0    13453\n",
      "NaN     2780\n",
      "9.0     1540\n",
      "3.0      761\n",
      "2.0      296\n",
      "4.0      109\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract the target variable from the data (EVOLUCAO column)\n",
    "\n",
    "1- cura\n",
    "2- óbito pelo\n",
    "agravo\n",
    "3- óbito por outras\n",
    "causas\n",
    "4- óbito em\n",
    "investigação\n",
    "9- ignorado\n",
    "\n",
    "Remove rows where the target variable is Nan, 3 (death by other causes), 4 (under investigation) or 9 (ignored)\n",
    "Only keep rows where the target variable is 1 (cure) or 2 (death by the disease)\n",
    "\"\"\"\n",
    "\n",
    "##############################################################\n",
    "# Should we drop rows where the target variable is 3 or 4?\n",
    "# Dropping them results in a very unbalanced dataset\n",
    "##############################################################\n",
    "\n",
    "# Print the value counts for the 'EVOLUCAO' column in X_train\n",
    "print(\"\\nValues for 'EVOLUCAO' in X_train:\")\n",
    "print(X_train[\"EVOLUCAO\"].value_counts(dropna=False))\n",
    "\n",
    "# Print the value counts for the 'EVOLUCAO' column in X_test\n",
    "print(\"\\nValues for 'EVOLUCAO' in X_test:\")\n",
    "print(X_test[\"EVOLUCAO\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values for 'EVOLUCAO' in y_train:\n",
      "EVOLUCAO\n",
      "0    15203\n",
      "1      251\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Values for 'EVOLUCAO' in y_test:\n",
      "EVOLUCAO\n",
      "0    13453\n",
      "1      296\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter the data to include only the rows where the target variable is 1 or 2\n",
    "X_train = X_train[(X_train['EVOLUCAO'] == 1) | (X_train['EVOLUCAO'] == 1.0) | (X_train['EVOLUCAO'] == 2) | (X_train['EVOLUCAO'] == 2.0)]\n",
    "X_test = X_test[(X_test['EVOLUCAO'] == 1) | (X_test['EVOLUCAO'] == 1.0) | (X_test['EVOLUCAO'] == 2) | (X_test['EVOLUCAO'] == 2.0)]\n",
    "\n",
    "# Remove the 'EVOLUCAO' column from X_train and save it in y_train\n",
    "y_train = X_train.pop(\"EVOLUCAO\")\n",
    "\n",
    "# Remove the 'EVOLUCAO' column from X_test and save it in y_test\n",
    "y_test = X_test.pop(\"EVOLUCAO\")\n",
    "\n",
    "# Change the target variable to 0 for cure and 1 for death\n",
    "y_train = y_train.map({1: 0, 2: 1})\n",
    "y_test = y_test.map({1: 0, 2: 1})\n",
    "\n",
    "# Print the value counts for the 'EVOLUCAO' column in y_train\n",
    "print(\"\\nValues for 'EVOLUCAO' in y_train:\")\n",
    "print(y_train.value_counts(dropna=True))\n",
    "\n",
    "# Optionally, also print the value counts for y_test\n",
    "print(\"\\nValues for 'EVOLUCAO' in y_test:\")\n",
    "print(y_test.value_counts(dropna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of missing values in X_train:\n",
      "Unnamed: 0    0.000000\n",
      "TP_NOT        0.000000\n",
      "ID_AGRAVO     0.000000\n",
      "DT_NOTIFIC    0.000000\n",
      "SEM_NOT       0.000000\n",
      "                ...   \n",
      "PLAQ_MENOR    1.000000\n",
      "CON_FHD       1.000000\n",
      "COMPLICA      1.000000\n",
      "TP_SISTEMA    0.016954\n",
      "NDUPLIC_N     0.999806\n",
      "Length: 116, dtype: float64\n",
      "\n",
      "Percentage of missing values in X_test:\n",
      "Unnamed: 0    0.000000\n",
      "TP_NOT        0.000000\n",
      "ID_AGRAVO     0.000000\n",
      "DT_NOTIFIC    0.000000\n",
      "SEM_NOT       0.000000\n",
      "                ...   \n",
      "PLAQ_MENOR    1.000000\n",
      "CON_FHD       1.000000\n",
      "COMPLICA      1.000000\n",
      "TP_SISTEMA    0.020874\n",
      "NDUPLIC_N     0.999273\n",
      "Length: 116, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Identify and remove columns with more than missing_values_threshold missing values in X_train and X_test\n",
    "\"\"\"\n",
    "\n",
    "missing_values_threshold = 0.80\n",
    "assert 0.0 <= missing_values_threshold <= 1.0, \"missing_values_threshold must be between 0 and 1\"\n",
    "\n",
    "# Compute the percentage of missing values in each column of X_train\n",
    "missing_values_train = X_train.isnull().mean()\n",
    "missing_values_test = X_test.isnull().mean()\n",
    "missing_values_mean = (missing_values_train + missing_values_test) / 2\n",
    "\n",
    "# Print the percentage of missing values in each column of X_train\n",
    "print(\"\\nPercentage of missing values in X_train:\")\n",
    "print(missing_values_train)\n",
    "\n",
    "# Print the percentage of missing values in each column of X_test\n",
    "print(\"\\nPercentage of missing values in X_test:\")\n",
    "print(missing_values_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped columns (more than 80.0% missing):\n",
      "['DT_CHIK_S2', 'DT_PRNT', 'DT_SORO', 'RESUL_SORO', 'DT_NS1', 'RESUL_NS1', 'DT_VIRAL', 'DT_PCR', 'SOROTIPO', 'HISTOPA_N', 'IMUNOH_N', 'DOENCA_TRA', 'DT_OBITO', 'ALRM_HIPOT', 'ALRM_PLAQ', 'ALRM_VOM', 'ALRM_SANG', 'ALRM_HEMAT', 'ALRM_ABDOM', 'ALRM_LETAR', 'ALRM_HEPAT', 'ALRM_LIQ', 'DT_ALRM', 'GRAV_PULSO', 'GRAV_CONV', 'GRAV_ENCH', 'GRAV_INSUF', 'GRAV_TAQUI', 'GRAV_EXTRE', 'GRAV_HIPOT', 'GRAV_HEMAT', 'GRAV_MELEN', 'GRAV_METRO', 'GRAV_SANG', 'GRAV_AST', 'GRAV_MIOC', 'GRAV_CONSC', 'GRAV_ORGAO', 'DT_GRAV', 'MANI_HEMOR', 'EPISTAXE', 'GENGIVO', 'METRO', 'PETEQUIAS', 'HEMATURA', 'SANGRAM', 'LACO_N', 'PLASMATICO', 'EVIDENCIA', 'PLAQ_MENOR', 'CON_FHD', 'COMPLICA', 'NDUPLIC_N']\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = missing_values_train[missing_values_mean > missing_values_threshold].index\n",
    "\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=cols_to_drop)\n",
    "print(f\"\\nDropped columns (more than {missing_values_threshold * 100}% missing):\")\n",
    "print(list(cols_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the filtered data to a new CSV file in the cleaned_path directory\n",
    "\"\"\"\n",
    "\n",
    "# Save the filtered data to a new CSV file in the cleaned_path directory\n",
    "X_train.to_csv(f'{cleaned_path}X_train.csv', index=False)\n",
    "y_train.to_csv(f'{cleaned_path}y_train.csv', index=False)\n",
    "\n",
    "X_test.to_csv(f'{cleaned_path}X_test.csv', index=False)\n",
    "y_test.to_csv(f'{cleaned_path}y_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
