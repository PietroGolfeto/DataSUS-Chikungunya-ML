{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data\n",
    "\n",
    "# SINAN DataSUS CSV files path (modify to match your file path)\n",
    "sinan_path = os.path.expanduser('~/Desktop/DataSUS-Chikungunya-ML/source/csv/')\n",
    "\n",
    "# Cleaned CSV files path (modify to match your file path)\n",
    "cleaned_path = os.path.expanduser('~/Desktop/DataSUS-Chikungunya-ML/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns common to all files:\n",
      "{'DT_INVEST', 'COUFINF', 'VOMITO', 'HOSPITALIZ', 'PETEQUIA_N', 'DT_SIN_PRI', 'GRAV_EXTRE', 'GRAV_HEMAT', 'DT_PRNT', 'NU_ANO', 'DT_NOTIFIC', 'CEFALEIA', 'ALRM_HIPOT', 'RES_CHIKS2', 'CS_GESTANT', 'EVIDENCIA', 'SEM_PRI', 'ID_REGIONA', 'SG_UF_NOT', 'RENAL', 'RESUL_VI_N', 'CLASSI_FIN', 'GRAV_CONSC', 'GRAV_ENCH', 'LACO_N', 'CS_SEXO', 'COMUNINF', 'HISTOPA_N', 'DT_CHIK_S2', 'ALRM_HEPAT', 'SOROTIPO', 'ID_PAIS', 'HIPERTENSA', 'ACIDO_PEPT', 'DT_NS1', 'NU_IDADE_N', 'AUTO_IMUNE', 'NAUSEA', 'ID_MUNICIP', 'ALRM_PLAQ', 'RESUL_PRNT', 'PLASMATICO', 'TP_SISTEMA', 'DT_GRAV', 'DT_CHIK_S1', 'GRAV_MIOC', 'GRAV_INSUF', 'COMPLICA', 'ID_AGRAVO', 'ALRM_LIQ', 'LACO', 'IMUNOH_N', 'CONJUNTVIT', 'HEPATOPAT', 'ID_UNIDADE', 'GRAV_SANG', 'DT_INTERNA', 'GRAV_ORGAO', 'GRAV_MELEN', 'DT_OBITO', 'CLINC_CHIK', 'ID_MN_RESI', 'DIABETES', 'GRAV_TAQUI', 'RESUL_PCR_', 'DT_ENCERRA', 'GRAV_HIPOT', 'SEM_NOT', 'TP_NOT', 'ALRM_VOM', 'EXANTEMA', 'GENGIVO', 'UF', 'ARTRITE', 'MANI_HEMOR', 'SANGRAM', 'GRAV_CONV', 'HEMATURA', 'PLAQ_MENOR', 'PETEQUIAS', 'MIALGIA', 'Unnamed: 0', 'ALRM_LETAR', 'ID_RG_RESI', 'FEBRE', 'COPAISINF', 'DOENCA_TRA', 'RES_CHIKS1', 'DOR_COSTAS', 'ALRM_ABDOM', 'GRAV_PULSO', 'DT_ALRM', 'ALRM_SANG', 'LEUCOPENIA', 'EPISTAXE', 'CS_RACA', 'CRITERIO', 'ARTRALGIA', 'TPAUTOCTO', 'METRO', 'RESUL_NS1', 'ALRM_HEMAT', 'DT_SORO', 'GRAV_METRO', 'HEMATOLOG', 'RESUL_SORO', 'CON_FHD', 'GRAV_AST', 'DT_PCR', 'MUNICIPIO', 'SG_UF', 'DOR_RETRO', 'CS_ESCOL_N', 'DT_VIRAL', 'ID_OCUPA_N', 'NDUPLIC_N', 'EVOLUCAO'}\n",
      "\n",
      "Columns that are not common among all files:\n",
      "Column 'FLXRECEBI' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'NU_LOTE_I' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'DT_DIGITA' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'CS_FLXRET' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'ANO_NASC' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'MIGRADO_W' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24', 'CHIKBR25'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the Chikungunya data from the CSV files\n",
    "The CSV files are named CHIKBRYY.csv, where YY is the last two digits of the year\n",
    "The files are stored in the ~/Downloads/dbc2csv/source/csv/ directory\n",
    "The columns in the CSV files are not consistent across all years\n",
    "We want to identify the columns that are common to all files\n",
    "\"\"\"\n",
    "\n",
    "# List of last two digits of years for which we have CSV files (2018 to 2024)\n",
    "start_year = 18\n",
    "end_year = 25\n",
    "assert start_year < end_year, \"Start year must be less than end year\"\n",
    "years = range(start_year, end_year + 1)\n",
    "\n",
    "# Dictionary to store the columns for each file\n",
    "file_columns = {}\n",
    "\n",
    "# Loop through each year, build the filename, and read the CSV\n",
    "for year in years:\n",
    "    file_name = f'{sinan_path}CHIKBR{str(year)}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_name, low_memory=False)\n",
    "        # Save the set of columns for this file\n",
    "        file_columns[file_name] = set(df.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "\n",
    "# Ensure we have loaded at least one file before proceeding\n",
    "assert file_columns, \"No files were loaded. Please check your file paths.\"\n",
    "\n",
    "# Find common columns: the intersection of columns across all files\n",
    "common_columns = set.intersection(*file_columns.values())\n",
    "print(\"\\nColumns common to all files:\")\n",
    "print(common_columns)\n",
    "\n",
    "# Compute the union of all columns (all columns that appear in any file)\n",
    "all_columns = set.union(*file_columns.values())\n",
    "\n",
    "# For columns that are not common, print which files have them and which don't.\n",
    "print(\"\\nColumns that are not common among all files:\")\n",
    "for col in all_columns - common_columns:\n",
    "    # Extract base name (e.g., CHIKBR21) from each file path\n",
    "    files_with = [os.path.splitext(os.path.basename(fname))[0] \n",
    "                    for fname, cols in file_columns.items() if col in cols]\n",
    "    files_without = [os.path.splitext(os.path.basename(fname))[0] \n",
    "                        for fname, cols in file_columns.items() if col not in cols]\n",
    "    print(f\"Column '{col}' is present in files: {files_with} and missing in files: {files_without}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the dataset: (1500209, 116)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove columns that are not common to all files\n",
    "Create a new DataFrame with only the common columns\n",
    "Concatenate all the DataFrames without the last {test_years} years into a single DataFrame called X_train\n",
    "Concatenate the DataFrames from the last {test_years} years into a single DataFrame called X_test\n",
    "\"\"\"\n",
    "\n",
    "# Number of years to use for testing\n",
    "test_years = 2\n",
    "assert common_columns, \"No common columns found. Please check your file paths.\"\n",
    "assert len(years) > test_years, \"At least {test_years + 1}  years of data are required.\"\n",
    "\n",
    "# Load data for all years and save in X dataset\n",
    "X = []\n",
    "for year in years:\n",
    "    file_name = f'{sinan_path}CHIKBR{str(year)}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_name, low_memory=False, usecols=common_columns)\n",
    "        if \"Unnamed: 0\" in df.columns:\n",
    "            df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "        X.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "        \n",
    "X = pd.concat(X, ignore_index=True)\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(f\"\\nShape of the dataset: {X.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the train and test data for each year, keeping only the common columns\n",
    "# X_train = []\n",
    "# X_test = []\n",
    "\n",
    "# for year in years:\n",
    "#     file_name = f'{sinan_path}CHIKBR{str(year)}.csv'\n",
    "#     try:\n",
    "#         df = pd.read_csv(file_name, usecols=common_columns, low_memory=False)\n",
    "#         # Drop the \"Unnamed: 0\" column if present\n",
    "#         if \"Unnamed: 0\" in df.columns:\n",
    "#             df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "#         if year < years[-test_years]:\n",
    "#             X_train.append(df)\n",
    "#         else:\n",
    "#             X_test.append(df)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file_name}: {e}\")\n",
    "\n",
    "# # Concatenate all the DataFrames into a single train and test DataFrame\n",
    "# X_train = pd.concat(X_train, ignore_index=True)\n",
    "# X_test = pd.concat(X_test, ignore_index=True)\n",
    "\n",
    "# # Display the shape of the concatenated DataFrame\n",
    "# print(\"\\nShape of the concatenated X_train DataFrame: \", X_train.shape)\n",
    "# print(\"Shape of the concatenated X_test DataFrame: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in the 'HOSPITALIZ' column:\n",
      "HOSPITALIZ\n",
      "2.0    835678\n",
      "2       73033\n",
      "1.0     42802\n",
      "9.0     30132\n",
      "1        3629\n",
      "9        2885\n",
      "           1\n",
      "Ø           1\n",
      "J           1\n",
      "ï           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Shape of the filtered X DataFrame:  (42802, 116)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Filter the data to include only the rows where the pacient was hospitalized (\"HOSPITALIZ\" column is equal to 1 or to 1.0)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nUnique values in the 'HOSPITALIZ' column:\")\n",
    "print(X['HOSPITALIZ'].value_counts())\n",
    "\n",
    "# Filter the data to include only the rows where the patient was hospitalized\n",
    "X = X[(X['HOSPITALIZ'] == 1) | (X['HOSPITALIZ'] == 1.0)]\n",
    "\n",
    "# Display the shape of the filtered DataFrame\n",
    "print(\"\\nShape of the filtered X DataFrame: \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values for 'EVOLUCAO' in X:\n",
      "EVOLUCAO\n",
      "1.0    29833\n",
      "NaN     7354\n",
      "9.0     3186\n",
      "3.0     1646\n",
      "2.0      565\n",
      "4.0      218\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract the target variable from the data (EVOLUCAO column)\n",
    "\n",
    "1- cura\n",
    "2- óbito pelo\n",
    "agravo\n",
    "3- óbito por outras\n",
    "causas\n",
    "4- óbito em\n",
    "investigação\n",
    "9- ignorado\n",
    "\n",
    "Remove rows where the target variable is Nan, 3 (death by other causes), 4 (under investigation) or 9 (ignored)\n",
    "Only keep rows where the target variable is 1 (cure) or 2 (death by the disease)\n",
    "\"\"\"\n",
    "\n",
    "##############################################################\n",
    "# Should we drop rows where the target variable is 3 or 4?\n",
    "# Dropping them results in a very unbalanced dataset\n",
    "##############################################################\n",
    "\n",
    "# Print the value counts for the 'EVOLUCAO' column in X\n",
    "print(\"\\nValues for 'EVOLUCAO' in X:\")\n",
    "print(X[\"EVOLUCAO\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to include only the rows where the target variable is 1 or 2\n",
    "X = X[(X['EVOLUCAO'] == 1.0) | (X['EVOLUCAO'] == 2.0) | (X['EVOLUCAO'] == 3.0) | (X['EVOLUCAO'] == 4.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped constant columns: ['TP_NOT', 'ID_AGRAVO', 'ID_PAIS', 'HISTOPA_N', 'IMUNOH_N', 'HOSPITALIZ', 'TP_SISTEMA', 'NDUPLIC_N']\n"
     ]
    }
   ],
   "source": [
    "# Remove columns where all values are the same in X (constant columns)\n",
    "constant_columns = [col for col in X.columns if X[col].nunique() == 1]\n",
    "\n",
    "# Drop these constant columns from X\n",
    "X = X.drop(columns=constant_columns)\n",
    "\n",
    "print(\"\\nDropped constant columns:\", constant_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of missing values in X:\n",
      "DT_NOTIFIC    0.0\n",
      "SEM_NOT       0.0\n",
      "NU_ANO        0.0\n",
      "SG_UF_NOT     0.0\n",
      "ID_MUNICIP    0.0\n",
      "             ... \n",
      "PLASMATICO    1.0\n",
      "EVIDENCIA     1.0\n",
      "PLAQ_MENOR    1.0\n",
      "CON_FHD       1.0\n",
      "COMPLICA      1.0\n",
      "Length: 108, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Identify and remove columns with more than missing_values_threshold missing values in X\n",
    "\"\"\"\n",
    "\n",
    "missing_values_threshold = 0.01\n",
    "assert 0.0 <= missing_values_threshold <= 1.0, \"missing_values_threshold must be between 0 and 1\"\n",
    "\n",
    "# Compute the percentage of missing values in each column of X\n",
    "missing_values = X.isnull().mean()\n",
    "\n",
    "# Print the percentage of missing values in each column of X\n",
    "print(\"\\nPercentage of missing values in X:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped columns (more than 1.0% missing):\n",
      "['ID_REGIONA', 'ID_UNIDADE', 'CS_ESCOL_N', 'ID_RG_RESI', 'ID_OCUPA_N', 'DT_CHIK_S1', 'DT_CHIK_S2', 'DT_PRNT', 'RES_CHIKS1', 'RES_CHIKS2', 'RESUL_PRNT', 'DT_SORO', 'RESUL_SORO', 'DT_NS1', 'RESUL_NS1', 'DT_VIRAL', 'RESUL_VI_N', 'DT_PCR', 'RESUL_PCR_', 'SOROTIPO', 'DT_INTERNA', 'UF', 'MUNICIPIO', 'TPAUTOCTO', 'COUFINF', 'COPAISINF', 'COMUNINF', 'DOENCA_TRA', 'CLINC_CHIK', 'DT_OBITO', 'DT_ENCERRA', 'ALRM_HIPOT', 'ALRM_PLAQ', 'ALRM_VOM', 'ALRM_SANG', 'ALRM_HEMAT', 'ALRM_ABDOM', 'ALRM_LETAR', 'ALRM_HEPAT', 'ALRM_LIQ', 'DT_ALRM', 'GRAV_PULSO', 'GRAV_CONV', 'GRAV_ENCH', 'GRAV_INSUF', 'GRAV_TAQUI', 'GRAV_EXTRE', 'GRAV_HIPOT', 'GRAV_HEMAT', 'GRAV_MELEN', 'GRAV_METRO', 'GRAV_SANG', 'GRAV_AST', 'GRAV_MIOC', 'GRAV_CONSC', 'GRAV_ORGAO', 'DT_GRAV', 'MANI_HEMOR', 'EPISTAXE', 'GENGIVO', 'METRO', 'PETEQUIAS', 'HEMATURA', 'SANGRAM', 'LACO_N', 'PLASMATICO', 'EVIDENCIA', 'PLAQ_MENOR', 'CON_FHD', 'COMPLICA']\n",
      "\n",
      "Shape of the filtered X DataFrame:  (32262, 38)\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = missing_values[missing_values > missing_values_threshold].index\n",
    "\n",
    "X = X.drop(columns=cols_to_drop)\n",
    "print(f\"\\nDropped columns (more than {missing_values_threshold * 100}% missing):\")\n",
    "print(list(cols_to_drop))\n",
    "\n",
    "# Display the shape of the filtered DataFrame\n",
    "print(\"\\nShape of the filtered X DataFrame: \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in X:  Index(['DT_NOTIFIC', 'SEM_NOT', 'NU_ANO', 'SG_UF_NOT', 'ID_MUNICIP',\n",
      "       'DT_SIN_PRI', 'SEM_PRI', 'NU_IDADE_N', 'CS_SEXO', 'CS_GESTANT',\n",
      "       'CS_RACA', 'SG_UF', 'ID_MN_RESI', 'DT_INVEST', 'FEBRE', 'MIALGIA',\n",
      "       'CEFALEIA', 'EXANTEMA', 'VOMITO', 'NAUSEA', 'DOR_COSTAS', 'CONJUNTVIT',\n",
      "       'ARTRITE', 'ARTRALGIA', 'PETEQUIA_N', 'LEUCOPENIA', 'LACO', 'DOR_RETRO',\n",
      "       'DIABETES', 'HEMATOLOG', 'HEPATOPAT', 'RENAL', 'HIPERTENSA',\n",
      "       'ACIDO_PEPT', 'AUTO_IMUNE', 'CLASSI_FIN', 'CRITERIO', 'EVOLUCAO'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in X: \", X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete over 20% missing: 45 features left\n",
    "\n",
    "Delete over 10% missing: 38 features left\n",
    "\n",
    "Delete over 5% missing: 38 features left\n",
    "\n",
    "Delete over 2% missing: 38 features left\n",
    "\n",
    "Delete over 1% missing: 37 features left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DT_NOTIFIC</th>\n",
       "      <th>SEM_NOT</th>\n",
       "      <th>NU_ANO</th>\n",
       "      <th>SG_UF_NOT</th>\n",
       "      <th>ID_MUNICIP</th>\n",
       "      <th>DT_SIN_PRI</th>\n",
       "      <th>SEM_PRI</th>\n",
       "      <th>NU_IDADE_N</th>\n",
       "      <th>CS_SEXO</th>\n",
       "      <th>CS_GESTANT</th>\n",
       "      <th>...</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>HEMATOLOG</th>\n",
       "      <th>HEPATOPAT</th>\n",
       "      <th>RENAL</th>\n",
       "      <th>HIPERTENSA</th>\n",
       "      <th>ACIDO_PEPT</th>\n",
       "      <th>AUTO_IMUNE</th>\n",
       "      <th>CLASSI_FIN</th>\n",
       "      <th>CRITERIO</th>\n",
       "      <th>EVOLUCAO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-07-06</td>\n",
       "      <td>201827</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>120040</td>\n",
       "      <td>2018-07-05</td>\n",
       "      <td>201827</td>\n",
       "      <td>4010.0</td>\n",
       "      <td>M</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2018-06-29</td>\n",
       "      <td>201826</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>120040</td>\n",
       "      <td>2018-06-24</td>\n",
       "      <td>201826</td>\n",
       "      <td>4029.0</td>\n",
       "      <td>F</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>201838</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>120040</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>201838</td>\n",
       "      <td>4011.0</td>\n",
       "      <td>F</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>201839</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>120040</td>\n",
       "      <td>2018-08-24</td>\n",
       "      <td>201834</td>\n",
       "      <td>4005.0</td>\n",
       "      <td>F</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2018-08-16</td>\n",
       "      <td>201833</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>120040</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>201833</td>\n",
       "      <td>4011.0</td>\n",
       "      <td>F</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DT_NOTIFIC  SEM_NOT  NU_ANO  SG_UF_NOT  ID_MUNICIP  DT_SIN_PRI  SEM_PRI  \\\n",
       "35   2018-07-06   201827    2018         12      120040  2018-07-05   201827   \n",
       "51   2018-06-29   201826    2018         12      120040  2018-06-24   201826   \n",
       "93   2018-09-19   201838    2018         12      120040  2018-09-17   201838   \n",
       "115  2018-09-24   201839    2018         12      120040  2018-08-24   201834   \n",
       "141  2018-08-16   201833    2018         12      120040  2018-08-13   201833   \n",
       "\n",
       "     NU_IDADE_N CS_SEXO CS_GESTANT  ... DIABETES HEMATOLOG HEPATOPAT RENAL  \\\n",
       "35       4010.0       M        6.0  ...      2.0       2.0       2.0   2.0   \n",
       "51       4029.0       F        5.0  ...      1.0       2.0       2.0   2.0   \n",
       "93       4011.0       F        5.0  ...      2.0       2.0       2.0   2.0   \n",
       "115      4005.0       F        6.0  ...      2.0       2.0       2.0   2.0   \n",
       "141      4011.0       F        5.0  ...      2.0       2.0       2.0   2.0   \n",
       "\n",
       "    HIPERTENSA ACIDO_PEPT AUTO_IMUNE CLASSI_FIN CRITERIO EVOLUCAO  \n",
       "35         2.0        2.0        2.0       13.0      1.0      1.0  \n",
       "51         1.0        2.0        2.0       13.0      1.0      1.0  \n",
       "93         2.0        2.0        2.0       13.0      2.0      1.0  \n",
       "115        2.0        2.0        2.0       13.0      1.0      1.0  \n",
       "141        2.0        2.0        2.0       13.0      1.0      1.0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in column 'SG_UF_NOT' in X:\n",
      "\n",
      "SG_UF_NOT\n",
      "35    4568\n",
      "31    4541\n",
      "23    3138\n",
      "33    2360\n",
      "50    2191\n",
      "29    1965\n",
      "26    1506\n",
      "21    1275\n",
      "24    1205\n",
      "25    1121\n",
      "51    1078\n",
      "17     926\n",
      "52     795\n",
      "22     745\n",
      "41     716\n",
      "27     700\n",
      "11     623\n",
      "15     610\n",
      "32     483\n",
      "28     394\n",
      "13     381\n",
      "43     304\n",
      "53     259\n",
      "14     150\n",
      "16      92\n",
      "42      80\n",
      "12      56\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count values in column \"SG_UF_NOT\" in X\n",
    "print(\"Values in column 'SG_UF_NOT' in X:\\n\")\n",
    "print(X[\"SG_UF_NOT\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary definition of the Brazilian states\n",
    "\n",
    "# Código   UF\t\tSigla\n",
    "# 11\tRondônia\tRO\n",
    "# 12\tAcre\tAC\n",
    "# 13\tAmazonas\tAM\n",
    "# 14\tRoraima\tRR\n",
    "# 15\tPará\tPA\n",
    "# 16\tAmapá\tAP\n",
    "# 17\tTocantins\tTO\n",
    "# 21\tMaranhão\tMA\n",
    "# 22\tPiauí\tPI\n",
    "# 23\tCeará\tCE\n",
    "# 24\tRio Grande do Norte\tRN\n",
    "# 25\tParaíba\tPB\n",
    "# 26\tPernambuco\tPE\n",
    "# 27\tAlagoas\tAL\n",
    "# 28\tSergipe\tSE\n",
    "# 29\tBahia\tBA\n",
    "# 31\tMinas Gerais\tMG\n",
    "# 32\tEspírito Santo\tES\n",
    "# 33\tRio de Janeiro\tRJ\n",
    "# 35\tSão Paulo\tSP\n",
    "# 41\tParaná\tPR\n",
    "# 42\tSanta Catarina\tSC\n",
    "# 43\tRio Grande do Sul (*)\tRS\n",
    "# 50\tMato Grosso do Sul\tMS\n",
    "# 51\tMato Grosso\tMT\n",
    "# 52\tGoiás\tGO\n",
    "# 53\tDistrito Federal\tDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map \"SG_UF_NOT\" value to the region of Brazil as a new one-hot column use these values:\n",
    "# \"Norte\": [11, 12, 13, 14, 15, 16, 17],\n",
    "# \"Nordeste\": [21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
    "# \"Centro-Oeste\": [50, 51, 52, 53],\n",
    "# \"Sudeste\": [31, 32, 33, 35],\n",
    "# \"Sul\": [41, 42, 43]\n",
    "\n",
    "X[\"REGION_NORTH\"] = X[\"SG_UF_NOT\"].isin([11, 12, 13, 14, 15, 16, 17]).astype(int)\n",
    "X[\"REGION_NORTHEAST\"] = X[\"SG_UF_NOT\"].isin([21, 22, 23, 24, 25, 26, 27, 28, 29]).astype(int)\n",
    "X[\"REGION_MIDWEST\"] = X[\"SG_UF_NOT\"].isin([50, 51, 52, 53]).astype(int)\n",
    "X[\"REGION_SOUTHEAST\"] = X[\"SG_UF_NOT\"].isin([31, 32, 33, 35]).astype(int)\n",
    "X[\"REGION_SOUTH\"] = X[\"SG_UF_NOT\"].isin([41, 42, 43]).astype(int)\n",
    "\n",
    "# Remove the \"SG_UF_NOT\" and \"ID_MUNICIP\" column from X\n",
    "X = X.drop(columns=[\"SG_UF_NOT\", \"ID_MUNICIP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values in each region in X:\n",
      "\n",
      "Region North:  2838\n",
      "Region Northeast:  12049\n",
      "Region Midwest:  4323\n",
      "Region Southeast:  11952\n",
      "Region South:  1100\n"
     ]
    }
   ],
   "source": [
    "# Count values in each region\n",
    "print(\"\\nValues in each region in X:\\n\")\n",
    "print(\"Region North: \", X[\"REGION_NORTH\"].sum())\n",
    "print(\"Region Northeast: \", X[\"REGION_NORTHEAST\"].sum())\n",
    "print(\"Region Midwest: \", X[\"REGION_MIDWEST\"].sum())\n",
    "print(\"Region Southeast: \", X[\"REGION_SOUTHEAST\"].sum())\n",
    "print(\"Region South: \", X[\"REGION_SOUTH\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"SG_UF\" and \"ID_MN_RESI\" columns from X (not useful, redundant values with \"SG_UF_NOT\" and \"ID_MUNICIP\")\n",
    "X = X.drop(columns=[\"SG_UF\", \"ID_MN_RESI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min and max values of 'TIME_DIFF_DAYS' in X:\n",
      "Min:  0\n",
      "Max:  1604\n"
     ]
    }
   ],
   "source": [
    "# Remove rows whose \"DT_SIN_PRI\" value is lower than 2017-07-01 (first symptoms reported before 2017-07-01 doesn't make sense to go to the hospital after 2018)\n",
    "previous_year = start_year - 1\n",
    "X = X[pd.to_datetime(X[\"DT_SIN_PRI\"]) >= pd.to_datetime(f\"20{previous_year}-07-01\")]\n",
    "\n",
    "# Save the difference in DAYS between \"DT_NOTIFIC\" and \"DT_SIN_PRI\" in a new column called \"TIME_DIFF_DAYS\"\n",
    "X[\"TIME_DIFF_DAYS\"] = (pd.to_datetime(X[\"DT_NOTIFIC\"]) - pd.to_datetime(X[\"DT_SIN_PRI\"])).dt.days\n",
    "\n",
    "# Print min and max values of \"TIME_DIFF_DAYS\"\n",
    "print(\"\\nMin and max values of 'TIME_DIFF_DAYS' in X:\")\n",
    "print(\"Min: \", X[\"TIME_DIFF_DAYS\"].min())\n",
    "print(\"Max: \", X[\"TIME_DIFF_DAYS\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows whose 'TIME_DIFF_DAYS' is greater than 180 days:  94\n"
     ]
    }
   ],
   "source": [
    "# Print number of rows whose \"TIME_DIFF_DAYS\" is greater than 180 days\n",
    "print(\"\\nNumber of rows whose 'TIME_DIFF_DAYS' is greater than 180 days: \", (X[\"TIME_DIFF_DAYS\"] > 180).sum())\n",
    "\n",
    "# Remove rows whose \"TIME_DIFF_DAYS\" is greater than 180 days\n",
    "X = X[X[\"TIME_DIFF_DAYS\"] <= 180]\n",
    "\n",
    "# Remove \"DT_SIN_PRI\" and \"SEM_PRI\" columns from X (not useful, redundant values with \"TIME_DIFF_DAYS\")\n",
    "X = X.drop(columns=[\"DT_SIN_PRI\", \"SEM_PRI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of SEM_NOT column:  int64\n",
      "Type of NU_ANO column:  int64\n",
      "Min value of SEM_NOT column:  201801\n",
      "Max value of SEM_NOT column:  202509\n",
      "Min value of DT_NOTIFIC column:  2017-12-31\n"
     ]
    }
   ],
   "source": [
    "# Use week and year info to create a new TIME column replacing DT_NOTIFIC, SEM_NOT and NU_ANO\n",
    "print(\"Type of SEM_NOT column: \", X[\"SEM_NOT\"].dtype)\n",
    "print(\"Type of NU_ANO column: \", X[\"NU_ANO\"].dtype)\n",
    "print(\"Min value of SEM_NOT column: \", X[\"SEM_NOT\"].min())\n",
    "print(\"Max value of SEM_NOT column: \", X[\"SEM_NOT\"].max())\n",
    "\n",
    "# Get min value from \"DT_NOTIFIC\" column\n",
    "print(\"Min value of DT_NOTIFIC column: \", X[\"DT_NOTIFIC\"].min())\n",
    "\n",
    "##########################################################################\n",
    "# TO DO\n",
    "# FIX WEEK CALCULATION FORMULA ERROR\n",
    "# Calculate the week from \"DT_NOTIFIC\" columns starting in 2017-12-31\n",
    "# start_notific = pd.to_datetime(X[\"DT_NOTIFIC\"].min())\n",
    "\n",
    "# X[\"TIME\"] = X[\"DT_NOTIFIC\"].apply(lambda x: (x - start_notific).days // 7)\n",
    "##########################################################################\n",
    "\n",
    "# Calculate the week and year from the SEM_NOT and NU_ANO columns\n",
    "week = X[\"SEM_NOT\"] % 100\n",
    "year = X[\"NU_ANO\"] - 2000 - start_year\n",
    "X[\"TIME\"] = year * 52 + week\n",
    "\n",
    "# Remove DT_NOTIFIC, SEM_NOT and NU_ANO columns\n",
    "X = X.drop(columns=[\"DT_NOTIFIC\", \"SEM_NOT\", \"NU_ANO\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values in 'CONFIRMED_CASE' column in X:\n",
      "CONFIRMED_CASE\n",
      "0.0    18045\n",
      "1.0    13890\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check all values in column \"CLASSI_FIN\"\n",
    "# 5.0 discarded\n",
    "# 13.0 confirmed case of Chikungunya\n",
    "\n",
    "# Map \"CLASSI_FIN\" value to 1 for confirmed cases and 0 for discarded cases\n",
    "X[\"CLASSI_FIN\"] = X[\"CLASSI_FIN\"].map({13.0: 1, 5.0: 0})\n",
    "\n",
    "# Rename \"CLASSI_FIN\" column to \"CONFIRMED_CASE\"\n",
    "X = X.rename(columns={\"CLASSI_FIN\": \"CONFIRMED_CASE\"})\n",
    "print(\"\\nValues in 'CONFIRMED_CASE' column in X:\")\n",
    "print(X[\"CONFIRMED_CASE\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"DT_INVEST\" column from X (not useful, redundant with \"TIME\" column)\n",
    "X = X.drop(columns=[\"DT_INVEST\"])\n",
    "\n",
    "# Rename \"NU_IDADE_N\" column to \"AGE\" column\n",
    "X = X.rename(columns={\"NU_IDADE_N\": \"AGE\"})\n",
    "\n",
    "# Remove rows whose \"AGE\" is less than 1000 (undefined, user error) or greater than 4150 (unrealistic, over 150 years)\n",
    "X = X[(X[\"AGE\"] >= 1000) & (X[\"AGE\"] <= 4150)]\n",
    "\n",
    "# Modify \"AGE\" column to have the age in years\n",
    "# If \"AGE\" is less than 4000, change to 0 years (hours, days or months old)\n",
    "# Else, subtract 4000 from the value to get the age in years\n",
    "\n",
    "X[\"AGE\"] = X[\"AGE\"].apply(lambda x: 0 if x < 4000 else x - 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AGE\n",
       "0.0      1508\n",
       "1.0       708\n",
       "11.0      587\n",
       "5.0       560\n",
       "8.0       555\n",
       "         ... \n",
       "105.0       1\n",
       "119.0       1\n",
       "101.0       1\n",
       "106.0       1\n",
       "104.0       1\n",
       "Name: count, Length: 105, dtype: int64"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common years are young people\n",
    "X[\"AGE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in 'CS_SEXO' column:  CS_SEXO\n",
      "F    16921\n",
      "M    15153\n",
      "I       10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print \"CS_SEXO\" column values\n",
    "print(\"Values in 'CS_SEXO' column: \", X[\"CS_SEXO\"].value_counts())\n",
    "\n",
    "# Keep rows whose \"CS_SEXO\" is \"M\" or \"F\"\n",
    "X = X[(X[\"CS_SEXO\"] == \"M\") | (X[\"CS_SEXO\"] == \"F\")]\n",
    "\n",
    "# Map \"CS_SEXO\" value to 1 if \"F\" and 0 if \"M\"\n",
    "X[\"CS_SEXO\"] = X[\"CS_SEXO\"].map({\"F\": 1, \"M\": 0})\n",
    "\n",
    "# Rename column to \"GENDER\"\n",
    "X = X.rename(columns={\"CS_SEXO\": \"GENDER\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in 'PREGNANT' column:  PREGNANT\n",
      "0.0    30945\n",
      "1.0     1126\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Map \"CS_GESTANT\" value to 1 if 1,2 or 3 (trimester of pregnancy)\n",
    "X[\"CS_GESTANT\"] = X[\"CS_GESTANT\"].map({1: 1, 2: 1, 3: 1, 4: 0, 5: 0, 6: 0, 9: 0})\n",
    "\n",
    "# Rename column to \"PREGNANT\"\n",
    "X = X.rename(columns={\"CS_GESTANT\": \"PREGNANT\"})\n",
    "print(\"Values in 'PREGNANT' column: \", X[\"PREGNANT\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in 'CS_RACA' column:  CS_RACA\n",
      "4.0    18106\n",
      "1.0     9048\n",
      "9.0     3049\n",
      "2.0     1400\n",
      "3.0      256\n",
      "5.0      215\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count values in \"CS_RACA\" column\n",
    "print(\"Values in 'CS_RACA' column: \", X[\"CS_RACA\"].value_counts())\n",
    "\n",
    "##############################################################\n",
    "# Bias question: does race influence the outcome of the disease?\n",
    "##############################################################\n",
    "\n",
    "# One-hot encode \"CS_RACA\" column, ignore value 9 (ignored)\n",
    "X[\"WHITE\"] = X[\"CS_RACA\"].isin([1]).astype(int)\n",
    "X[\"BLACK\"] = X[\"CS_RACA\"].isin([2]).astype(int)\n",
    "X[\"YELLOW\"] = X[\"CS_RACA\"].isin([3]).astype(int)\n",
    "X[\"BROWN\"] = X[\"CS_RACA\"].isin([4]).astype(int)\n",
    "X[\"INDIGENOUS\"] = X[\"CS_RACA\"].isin([5]).astype(int)\n",
    "\n",
    "# Remove \"CS_RACA\" column\n",
    "X = X.drop(columns=[\"CS_RACA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms_columns = [\"FEBRE\", \"MIALGIA\", \"CEFALEIA\", \"EXANTEMA\", \"VOMITO\", \"NAUSEA\", \"DOR_COSTAS\", \"CONJUNTVIT\", \"ARTRITE\", \"ARTRALGIA\", \"PETEQUIA_N\", \"LEUCOPENIA\", \"LACO\", \"DOR_RETRO\", \"DIABETES\", \"HEMATOLOG\", \"HEPATOPAT\", \"RENAL\", \"HIPERTENSA\", \"ACIDO_PEPT\", \"AUTO_IMUNE\"]\n",
    "\n",
    "# Map symptoms columns to 1 if the value is 1 else 0 (no symptom)\n",
    "for col in symptoms_columns:\n",
    "    X[col] = X[col].map({1: 1, 2: 0, 9: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "373\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "# Split the data into X_train, y_train, X_test, y_test\n",
    "# Use the last {test_years} years for testing and the rest for training\n",
    "max_train_week_value = (end_year - start_year - test_years) * 52\n",
    "print(max_train_week_value)\n",
    "print(X[\"TIME\"].max())\n",
    "\n",
    "X_train = X[X[\"TIME\"] <= max_train_week_value]\n",
    "X_test = X[X[\"TIME\"] > max_train_week_value]\n",
    "\n",
    "# Remove the 'EVOLUCAO' column from X_train and save it in y_train\n",
    "y_train = X_train.pop(\"EVOLUCAO\")\n",
    "\n",
    "# Remove the 'EVOLUCAO' column from X_test and save it in y_test\n",
    "y_test = X_test.pop(\"EVOLUCAO\")\n",
    "\n",
    "# Change the target variable to 0 for cure and 1 for death\n",
    "y_train = y_train.map({1: 0, 2: 1, 3:1, 4:1})\n",
    "y_test = y_test.map({1: 0, 2: 1, 3:1, 4:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values for 'EVOLUCAO' in y_train:\n",
      "EVOLUCAO\n",
      "0    15080\n",
      "1     1153\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Values for 'EVOLUCAO' in y_test:\n",
      "EVOLUCAO\n",
      "0    14608\n",
      "1     1233\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Values of the y_train and y_test\n",
    "print(\"\\nValues for 'EVOLUCAO' in y_train:\")\n",
    "print(y_train.value_counts(dropna=True))\n",
    "\n",
    "print(\"\\nValues for 'EVOLUCAO' in y_test:\")\n",
    "print(y_test.value_counts(dropna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>PREGNANT</th>\n",
       "      <th>FEBRE</th>\n",
       "      <th>MIALGIA</th>\n",
       "      <th>CEFALEIA</th>\n",
       "      <th>EXANTEMA</th>\n",
       "      <th>VOMITO</th>\n",
       "      <th>NAUSEA</th>\n",
       "      <th>DOR_COSTAS</th>\n",
       "      <th>...</th>\n",
       "      <th>REGION_MIDWEST</th>\n",
       "      <th>REGION_SOUTHEAST</th>\n",
       "      <th>REGION_SOUTH</th>\n",
       "      <th>TIME_DIFF_DAYS</th>\n",
       "      <th>TIME</th>\n",
       "      <th>WHITE</th>\n",
       "      <th>BLACK</th>\n",
       "      <th>YELLOW</th>\n",
       "      <th>BROWN</th>\n",
       "      <th>INDIGENOUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AGE  GENDER  PREGNANT  FEBRE  MIALGIA  CEFALEIA  EXANTEMA  VOMITO  \\\n",
       "35   10.0       0       0.0      1        1         1         1       0   \n",
       "51   29.0       1       0.0      1        1         1         1       0   \n",
       "93   11.0       1       0.0      1        1         1         0       0   \n",
       "115   5.0       1       0.0      1        0         1         0       0   \n",
       "141  11.0       1       0.0      1        1         1         0       0   \n",
       "\n",
       "     NAUSEA  DOR_COSTAS  ...  REGION_MIDWEST  REGION_SOUTHEAST  REGION_SOUTH  \\\n",
       "35        1           1  ...               0                 0             0   \n",
       "51        1           1  ...               0                 0             0   \n",
       "93        0           0  ...               0                 0             0   \n",
       "115       1           0  ...               0                 0             0   \n",
       "141       0           0  ...               0                 0             0   \n",
       "\n",
       "     TIME_DIFF_DAYS  TIME  WHITE  BLACK  YELLOW  BROWN  INDIGENOUS  \n",
       "35                1    27      0      0       0      1           0  \n",
       "51                5    26      0      0       0      1           0  \n",
       "93                2    38      0      0       0      1           0  \n",
       "115              31    39      0      0       0      1           0  \n",
       "141               3    33      0      0       0      1           0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the filtered data to a new CSV file in the cleaned_path directory\n",
    "\"\"\"\n",
    "\n",
    "# Save the filtered data to a new CSV file in the cleaned_path directory\n",
    "X_train.to_csv(f'{cleaned_path}X_train.csv', index=False)\n",
    "y_train.to_csv(f'{cleaned_path}y_train.csv', index=False)\n",
    "\n",
    "X_test.to_csv(f'{cleaned_path}X_test.csv', index=False)\n",
    "y_test.to_csv(f'{cleaned_path}y_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
