{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data\n",
    "\n",
    "# SINAN DataSUS CSV files path (modify to match your file path)\n",
    "sinan_path = os.path.expanduser('~/Desktop/DataSUS-Chikungunya-ML/source/csv/')\n",
    "\n",
    "# Cleaned CSV files path (modify to match your file path)\n",
    "cleaned_path = os.path.expanduser('~/Desktop/DataSUS-Chikungunya-ML/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns common to all files:\n",
      "{'DT_INVEST', 'COUFINF', 'VOMITO', 'HOSPITALIZ', 'PETEQUIA_N', 'GRAV_EXTRE', 'DT_SIN_PRI', 'GRAV_HEMAT', 'DT_PRNT', 'NU_ANO', 'DT_NOTIFIC', 'CEFALEIA', 'ALRM_HIPOT', 'RES_CHIKS2', 'CS_GESTANT', 'EVIDENCIA', 'SEM_PRI', 'ID_REGIONA', 'SG_UF_NOT', 'RENAL', 'RESUL_VI_N', 'CLASSI_FIN', 'GRAV_CONSC', 'GRAV_ENCH', 'LACO_N', 'CS_SEXO', 'COMUNINF', 'HISTOPA_N', 'DT_CHIK_S2', 'ALRM_HEPAT', 'SOROTIPO', 'ID_PAIS', 'HIPERTENSA', 'ACIDO_PEPT', 'DT_NS1', 'NU_IDADE_N', 'AUTO_IMUNE', 'NAUSEA', 'ID_MUNICIP', 'ALRM_PLAQ', 'RESUL_PRNT', 'PLASMATICO', 'TP_SISTEMA', 'DT_GRAV', 'DT_CHIK_S1', 'GRAV_MIOC', 'GRAV_INSUF', 'COMPLICA', 'ID_AGRAVO', 'ALRM_LIQ', 'LACO', 'IMUNOH_N', 'CONJUNTVIT', 'HEPATOPAT', 'ID_UNIDADE', 'GRAV_SANG', 'DT_INTERNA', 'GRAV_ORGAO', 'GRAV_MELEN', 'DT_OBITO', 'CLINC_CHIK', 'ID_MN_RESI', 'DIABETES', 'GRAV_TAQUI', 'RESUL_PCR_', 'DT_ENCERRA', 'GRAV_HIPOT', 'SEM_NOT', 'TP_NOT', 'ALRM_VOM', 'EXANTEMA', 'GENGIVO', 'UF', 'ARTRITE', 'MANI_HEMOR', 'SANGRAM', 'GRAV_CONV', 'HEMATURA', 'PLAQ_MENOR', 'PETEQUIAS', 'MIALGIA', 'Unnamed: 0', 'ALRM_LETAR', 'ID_RG_RESI', 'FEBRE', 'COPAISINF', 'DOENCA_TRA', 'RES_CHIKS1', 'DOR_COSTAS', 'ALRM_ABDOM', 'GRAV_PULSO', 'DT_ALRM', 'ALRM_SANG', 'LEUCOPENIA', 'EPISTAXE', 'CS_RACA', 'CRITERIO', 'ARTRALGIA', 'TPAUTOCTO', 'METRO', 'RESUL_NS1', 'ALRM_HEMAT', 'DT_SORO', 'GRAV_METRO', 'HEMATOLOG', 'RESUL_SORO', 'CON_FHD', 'GRAV_AST', 'DT_PCR', 'MUNICIPIO', 'SG_UF', 'DOR_RETRO', 'CS_ESCOL_N', 'DT_VIRAL', 'ID_OCUPA_N', 'NDUPLIC_N', 'EVOLUCAO'}\n",
      "\n",
      "Columns that are not common among all files:\n",
      "Column 'FLXRECEBI' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'NU_LOTE_I' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'DT_DIGITA' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'CS_FLXRET' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'ANO_NASC' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n",
      "Column 'MIGRADO_W' is present in files: ['CHIKBR21', 'CHIKBR22', 'CHIKBR23', 'CHIKBR24'] and missing in files: ['CHIKBR18', 'CHIKBR19', 'CHIKBR20']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the Chikungunya data from the CSV files\n",
    "The CSV files are named CHIKBRYY.csv, where YY is the last two digits of the year\n",
    "The files are stored in the ~/Downloads/dbc2csv/source/csv/ directory\n",
    "The columns in the CSV files are not consistent across all years\n",
    "We want to identify the columns that are common to all files\n",
    "\"\"\"\n",
    "\n",
    "# List of last two digits of years for which we have CSV files (2018 to 2024)\n",
    "start_year = 18\n",
    "end_year = 24\n",
    "assert start_year < end_year, \"Start year must be less than end year\"\n",
    "years = range(start_year, end_year + 1)\n",
    "\n",
    "# Dictionary to store the columns for each file\n",
    "file_columns = {}\n",
    "\n",
    "# Loop through each year, build the filename, and read the CSV\n",
    "for year in years:\n",
    "    file_name = f'{sinan_path}CHIKBR{str(year)}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_name, low_memory=False)\n",
    "        # Save the set of columns for this file\n",
    "        file_columns[file_name] = set(df.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "\n",
    "# Ensure we have loaded at least one file before proceeding\n",
    "assert file_columns, \"No files were loaded. Please check your file paths.\"\n",
    "\n",
    "# Find common columns: the intersection of columns across all files\n",
    "common_columns = set.intersection(*file_columns.values())\n",
    "print(\"\\nColumns common to all files:\")\n",
    "print(common_columns)\n",
    "\n",
    "# Compute the union of all columns (all columns that appear in any file)\n",
    "all_columns = set.union(*file_columns.values())\n",
    "\n",
    "# For columns that are not common, print which files have them and which don't.\n",
    "print(\"\\nColumns that are not common among all files:\")\n",
    "for col in all_columns - common_columns:\n",
    "    # Extract base name (e.g., CHIKBR21) from each file path\n",
    "    files_with = [os.path.splitext(os.path.basename(fname))[0] \n",
    "                    for fname, cols in file_columns.items() if col in cols]\n",
    "    files_without = [os.path.splitext(os.path.basename(fname))[0] \n",
    "                        for fname, cols in file_columns.items() if col not in cols]\n",
    "    print(f\"Column '{col}' is present in files: {files_with} and missing in files: {files_without}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the concatenated X_train DataFrame:  (802287, 116)\n",
      "Shape of the concatenated X_test DataFrame:  (650214, 116)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove columns that are not common to all files\n",
    "Create a new DataFrame with only the common columns\n",
    "Concatenate all the DataFrames without the last {test_years} years into a single DataFrame called X_train\n",
    "Concatenate the DataFrames from the last {test_years} years into a single DataFrame called X_test\n",
    "\"\"\"\n",
    "\n",
    "# Number of years to use for testing\n",
    "test_years = 2\n",
    "assert common_columns, \"No common columns found. Please check your file paths.\"\n",
    "assert len(years) > test_years, \"At least {test_years + 1}  years of data are required.\"\n",
    "\n",
    "# Load the train and test data for each year, keeping only the common columns\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for year in years:\n",
    "    file_name = f'{sinan_path}CHIKBR{str(year)}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_name, usecols=common_columns, low_memory=False)\n",
    "        # Drop the \"Unnamed: 0\" column if present\n",
    "        if \"Unnamed: 0\" in df.columns:\n",
    "            df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "        if year < years[-test_years]:\n",
    "            X_train.append(df)\n",
    "        else:\n",
    "            X_test.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "\n",
    "# Concatenate all the DataFrames into a single train and test DataFrame\n",
    "X_train = pd.concat(X_train, ignore_index=True)\n",
    "X_test = pd.concat(X_test, ignore_index=True)\n",
    "\n",
    "# Display the shape of the concatenated DataFrame\n",
    "print(\"\\nShape of the concatenated X_train DataFrame: \", X_train.shape)\n",
    "print(\"Shape of the concatenated X_test DataFrame: \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in the 'HOSPITALIZ' column:\n",
      "HOSPITALIZ\n",
      "2.0    393418\n",
      "2       73033\n",
      "1.0     21844\n",
      "9.0     15658\n",
      "1        3629\n",
      "9        2885\n",
      "           1\n",
      "Ø           1\n",
      "J           1\n",
      "ï           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Shape of the filtered X_train DataFrame:  (21844, 116)\n",
      "Shape of the filtered X_test DataFrame:  (18939, 116)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Filter the data to include only the rows where the pacient was hospitalized (\"HOSPITALIZ\" column is equal to 1 or to 1.0)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nUnique values in the 'HOSPITALIZ' column:\")\n",
    "print(X_train['HOSPITALIZ'].value_counts())\n",
    "\n",
    "# Filter the data to include only the rows where the patient was hospitalized\n",
    "X_train = X_train[(X_train['HOSPITALIZ'] == 1) | (X_train['HOSPITALIZ'] == 1.0)]\n",
    "X_test = X_test[(X_test['HOSPITALIZ'] == 1) | (X_test['HOSPITALIZ'] == 1.0)]\n",
    "\n",
    "# Display the shape of the filtered DataFrame\n",
    "print(\"\\nShape of the filtered X_train DataFrame: \", X_train.shape)\n",
    "print(\"Shape of the filtered X_test DataFrame: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values for 'EVOLUCAO' in X_train:\n",
      "EVOLUCAO\n",
      "1.0    15203\n",
      "NaN     3865\n",
      "9.0     1587\n",
      "3.0      858\n",
      "2.0      251\n",
      "4.0       80\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Values for 'EVOLUCAO' in X_test:\n",
      "EVOLUCAO\n",
      "1.0    13453\n",
      "NaN     2780\n",
      "9.0     1540\n",
      "3.0      761\n",
      "2.0      296\n",
      "4.0      109\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract the target variable from the data (EVOLUCAO column)\n",
    "\n",
    "1- cura\n",
    "2- óbito pelo\n",
    "agravo\n",
    "3- óbito por outras\n",
    "causas\n",
    "4- óbito em\n",
    "investigação\n",
    "9- ignorado\n",
    "\n",
    "Remove rows where the target variable is Nan, 3 (death by other causes), 4 (under investigation) or 9 (ignored)\n",
    "Only keep rows where the target variable is 1 (cure) or 2 (death by the disease)\n",
    "\"\"\"\n",
    "\n",
    "##############################################################\n",
    "# Should we drop rows where the target variable is 3 or 4?\n",
    "# Dropping them results in a very unbalanced dataset\n",
    "##############################################################\n",
    "\n",
    "# Print the value counts for the 'EVOLUCAO' column in X_train\n",
    "print(\"\\nValues for 'EVOLUCAO' in X_train:\")\n",
    "print(X_train[\"EVOLUCAO\"].value_counts(dropna=False))\n",
    "\n",
    "# Print the value counts for the 'EVOLUCAO' column in X_test\n",
    "print(\"\\nValues for 'EVOLUCAO' in X_test:\")\n",
    "print(X_test[\"EVOLUCAO\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to include only the rows where the target variable is 1 or 2\n",
    "X_train = X_train[(X_train['EVOLUCAO'] == 1.0) | (X_train['EVOLUCAO'] == 2.0) | (X_train['EVOLUCAO'] == 3.0) | (X_train['EVOLUCAO'] == 4.0)]\n",
    "X_test = X_test[(X_test['EVOLUCAO'] == 1.0) | (X_test['EVOLUCAO'] == 2.0) | (X_test['EVOLUCAO'] == 3.0) | (X_test['EVOLUCAO'] == 4.0)]\n",
    "\n",
    "# Remove the 'EVOLUCAO' column from X_train and save it in y_train\n",
    "y_train = X_train.pop(\"EVOLUCAO\")\n",
    "\n",
    "# Remove the 'EVOLUCAO' column from X_test and save it in y_test\n",
    "y_test = X_test.pop(\"EVOLUCAO\")\n",
    "\n",
    "# Change the target variable to 0 for cure and 1 for death\n",
    "y_train = y_train.map({1: 0, 2: 1, 3:1, 4:1})\n",
    "y_test = y_test.map({1: 0, 2: 1, 3:1, 4:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped constant columns: ['TP_NOT', 'ID_AGRAVO', 'ID_PAIS', 'HISTOPA_N', 'IMUNOH_N', 'HOSPITALIZ', 'TP_SISTEMA', 'NDUPLIC_N']\n"
     ]
    }
   ],
   "source": [
    "# Remove columns where all values are the same in X_train and X_test (constant columns)\n",
    "constant_columns = [col for col in X_train.columns if X_train[col].nunique() == 1]\n",
    "\n",
    "# Drop these constant columns from both X_train and X_test\n",
    "X_train = X_train.drop(columns=constant_columns)\n",
    "X_test = X_test.drop(columns=constant_columns, errors='ignore')\n",
    "\n",
    "print(\"\\nDropped constant columns:\", constant_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of missing values in X_train:\n",
      "DT_NOTIFIC    0.0\n",
      "SEM_NOT       0.0\n",
      "NU_ANO        0.0\n",
      "SG_UF_NOT     0.0\n",
      "ID_MUNICIP    0.0\n",
      "             ... \n",
      "PLASMATICO    1.0\n",
      "EVIDENCIA     1.0\n",
      "PLAQ_MENOR    1.0\n",
      "CON_FHD       1.0\n",
      "COMPLICA      1.0\n",
      "Length: 107, dtype: float64\n",
      "\n",
      "Percentage of missing values in X_test:\n",
      "DT_NOTIFIC    0.0\n",
      "SEM_NOT       0.0\n",
      "NU_ANO        0.0\n",
      "SG_UF_NOT     0.0\n",
      "ID_MUNICIP    0.0\n",
      "             ... \n",
      "PLASMATICO    1.0\n",
      "EVIDENCIA     1.0\n",
      "PLAQ_MENOR    1.0\n",
      "CON_FHD       1.0\n",
      "COMPLICA      1.0\n",
      "Length: 107, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Identify and remove columns with more than missing_values_threshold missing values in X_train and X_test\n",
    "\"\"\"\n",
    "\n",
    "missing_values_threshold = 0.01\n",
    "assert 0.0 <= missing_values_threshold <= 1.0, \"missing_values_threshold must be between 0 and 1\"\n",
    "\n",
    "# Compute the percentage of missing values in each column of X_train\n",
    "missing_values_train = X_train.isnull().mean()\n",
    "missing_values_test = X_test.isnull().mean()\n",
    "missing_values_mean = (missing_values_train + missing_values_test) / 2\n",
    "\n",
    "# Print the percentage of missing values in each column of X_train\n",
    "print(\"\\nPercentage of missing values in X_train:\")\n",
    "print(missing_values_train)\n",
    "\n",
    "# Print the percentage of missing values in each column of X_test\n",
    "print(\"\\nPercentage of missing values in X_test:\")\n",
    "print(missing_values_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped columns (more than 1.0% missing):\n",
      "['ID_REGIONA', 'ID_UNIDADE', 'CS_ESCOL_N', 'ID_RG_RESI', 'ID_OCUPA_N', 'DT_CHIK_S1', 'DT_CHIK_S2', 'DT_PRNT', 'RES_CHIKS1', 'RES_CHIKS2', 'RESUL_PRNT', 'DT_SORO', 'RESUL_SORO', 'DT_NS1', 'RESUL_NS1', 'DT_VIRAL', 'RESUL_VI_N', 'DT_PCR', 'RESUL_PCR_', 'SOROTIPO', 'DT_INTERNA', 'UF', 'MUNICIPIO', 'TPAUTOCTO', 'COUFINF', 'COPAISINF', 'COMUNINF', 'DOENCA_TRA', 'CLINC_CHIK', 'DT_OBITO', 'DT_ENCERRA', 'ALRM_HIPOT', 'ALRM_PLAQ', 'ALRM_VOM', 'ALRM_SANG', 'ALRM_HEMAT', 'ALRM_ABDOM', 'ALRM_LETAR', 'ALRM_HEPAT', 'ALRM_LIQ', 'DT_ALRM', 'GRAV_PULSO', 'GRAV_CONV', 'GRAV_ENCH', 'GRAV_INSUF', 'GRAV_TAQUI', 'GRAV_EXTRE', 'GRAV_HIPOT', 'GRAV_HEMAT', 'GRAV_MELEN', 'GRAV_METRO', 'GRAV_SANG', 'GRAV_AST', 'GRAV_MIOC', 'GRAV_CONSC', 'GRAV_ORGAO', 'DT_GRAV', 'MANI_HEMOR', 'EPISTAXE', 'GENGIVO', 'METRO', 'PETEQUIAS', 'HEMATURA', 'SANGRAM', 'LACO_N', 'PLASMATICO', 'EVIDENCIA', 'PLAQ_MENOR', 'CON_FHD', 'COMPLICA']\n",
      "\n",
      "Shape of the filtered X_train DataFrame:  (16392, 37)\n",
      "Shape of the filtered X_test DataFrame:  (14619, 37)\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = missing_values_train[missing_values_mean > missing_values_threshold].index\n",
    "\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=cols_to_drop)\n",
    "print(f\"\\nDropped columns (more than {missing_values_threshold * 100}% missing):\")\n",
    "print(list(cols_to_drop))\n",
    "\n",
    "# Display the shape of the filtered DataFrame\n",
    "print(\"\\nShape of the filtered X_train DataFrame: \", X_train.shape)\n",
    "print(\"Shape of the filtered X_test DataFrame: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values for 'EVOLUCAO' in y_train:\n",
      "EVOLUCAO\n",
      "0    15203\n",
      "1     1189\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Values for 'EVOLUCAO' in y_test:\n",
      "EVOLUCAO\n",
      "0    13453\n",
      "1     1166\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Values of the y_train and y_test\n",
    "print(\"\\nValues for 'EVOLUCAO' in y_train:\")\n",
    "print(y_train.value_counts(dropna=True))\n",
    "\n",
    "print(\"\\nValues for 'EVOLUCAO' in y_test:\")\n",
    "print(y_test.value_counts(dropna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in X_train:  Index(['DT_NOTIFIC', 'SEM_NOT', 'NU_ANO', 'SG_UF_NOT', 'ID_MUNICIP',\n",
      "       'DT_SIN_PRI', 'SEM_PRI', 'NU_IDADE_N', 'CS_SEXO', 'CS_GESTANT',\n",
      "       'CS_RACA', 'SG_UF', 'ID_MN_RESI', 'DT_INVEST', 'FEBRE', 'MIALGIA',\n",
      "       'CEFALEIA', 'EXANTEMA', 'VOMITO', 'NAUSEA', 'DOR_COSTAS', 'CONJUNTVIT',\n",
      "       'ARTRITE', 'ARTRALGIA', 'PETEQUIA_N', 'LEUCOPENIA', 'LACO', 'DOR_RETRO',\n",
      "       'DIABETES', 'HEMATOLOG', 'HEPATOPAT', 'RENAL', 'HIPERTENSA',\n",
      "       'ACIDO_PEPT', 'AUTO_IMUNE', 'CLASSI_FIN', 'CRITERIO'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in X_train: \", X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete over 20% missing: 45 features left\n",
    "\n",
    "Delete over 10% missing: 38 features left\n",
    "\n",
    "Delete over 5% missing: 38 features left\n",
    "\n",
    "Delete over 2% missing: 38 features left\n",
    "\n",
    "Delete over 1% missing: 37 features left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DT_NOTIFIC</th>\n",
       "      <th>SEM_NOT</th>\n",
       "      <th>NU_ANO</th>\n",
       "      <th>SG_UF_NOT</th>\n",
       "      <th>ID_MUNICIP</th>\n",
       "      <th>DT_SIN_PRI</th>\n",
       "      <th>SEM_PRI</th>\n",
       "      <th>NU_IDADE_N</th>\n",
       "      <th>CS_SEXO</th>\n",
       "      <th>CS_GESTANT</th>\n",
       "      <th>...</th>\n",
       "      <th>DOR_RETRO</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>HEMATOLOG</th>\n",
       "      <th>HEPATOPAT</th>\n",
       "      <th>RENAL</th>\n",
       "      <th>HIPERTENSA</th>\n",
       "      <th>ACIDO_PEPT</th>\n",
       "      <th>AUTO_IMUNE</th>\n",
       "      <th>CLASSI_FIN</th>\n",
       "      <th>CRITERIO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-07-06</td>\n",
       "      <td>201827</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>120040</td>\n",
       "      <td>2018-07-05</td>\n",
       "      <td>201827</td>\n",
       "      <td>4010.0</td>\n",
       "      <td>M</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2018-06-29</td>\n",
       "      <td>201826</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>120040</td>\n",
       "      <td>2018-06-24</td>\n",
       "      <td>201826</td>\n",
       "      <td>4029.0</td>\n",
       "      <td>F</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>201838</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>120040</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>201838</td>\n",
       "      <td>4011.0</td>\n",
       "      <td>F</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>201839</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>120040</td>\n",
       "      <td>2018-08-24</td>\n",
       "      <td>201834</td>\n",
       "      <td>4005.0</td>\n",
       "      <td>F</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2018-08-16</td>\n",
       "      <td>201833</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>120040</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>201833</td>\n",
       "      <td>4011.0</td>\n",
       "      <td>F</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DT_NOTIFIC  SEM_NOT  NU_ANO  SG_UF_NOT  ID_MUNICIP  DT_SIN_PRI  SEM_PRI  \\\n",
       "35   2018-07-06   201827    2018         12      120040  2018-07-05   201827   \n",
       "51   2018-06-29   201826    2018         12      120040  2018-06-24   201826   \n",
       "93   2018-09-19   201838    2018         12      120040  2018-09-17   201838   \n",
       "115  2018-09-24   201839    2018         12      120040  2018-08-24   201834   \n",
       "141  2018-08-16   201833    2018         12      120040  2018-08-13   201833   \n",
       "\n",
       "     NU_IDADE_N CS_SEXO CS_GESTANT  ... DOR_RETRO DIABETES HEMATOLOG  \\\n",
       "35       4010.0       M        6.0  ...       1.0      2.0       2.0   \n",
       "51       4029.0       F        5.0  ...       1.0      1.0       2.0   \n",
       "93       4011.0       F        5.0  ...       2.0      2.0       2.0   \n",
       "115      4005.0       F        6.0  ...       1.0      2.0       2.0   \n",
       "141      4011.0       F        5.0  ...       2.0      2.0       2.0   \n",
       "\n",
       "    HEPATOPAT RENAL HIPERTENSA ACIDO_PEPT AUTO_IMUNE CLASSI_FIN CRITERIO  \n",
       "35        2.0   2.0        2.0        2.0        2.0       13.0      1.0  \n",
       "51        2.0   2.0        1.0        2.0        2.0       13.0      1.0  \n",
       "93        2.0   2.0        2.0        2.0        2.0       13.0      2.0  \n",
       "115       2.0   2.0        2.0        2.0        2.0       13.0      1.0  \n",
       "141       2.0   2.0        2.0        2.0        2.0       13.0      1.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in column 'SG_UF_NOT' in X_train:\n",
      "\n",
      "SG_UF_NOT\n",
      "23    2184\n",
      "33    1953\n",
      "35    1744\n",
      "29    1106\n",
      "31    1086\n",
      "26     952\n",
      "24     856\n",
      "25     757\n",
      "21     646\n",
      "17     623\n",
      "27     533\n",
      "22     521\n",
      "32     483\n",
      "50     451\n",
      "15     410\n",
      "11     381\n",
      "52     282\n",
      "28     245\n",
      "51     238\n",
      "41     234\n",
      "43     229\n",
      "13     143\n",
      "53     118\n",
      "14      90\n",
      "42      47\n",
      "16      44\n",
      "12      36\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count values in column \"SG_UF_NOT\" in X_train\n",
    "print(\"Values in column 'SG_UF_NOT' in X_train:\\n\")\n",
    "print(X_train[\"SG_UF_NOT\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary definition of the Brazilian states\n",
    "\n",
    "# Código   UF\t\tSigla\n",
    "# 11\tRondônia\tRO\n",
    "# 12\tAcre\tAC\n",
    "# 13\tAmazonas\tAM\n",
    "# 14\tRoraima\tRR\n",
    "# 15\tPará\tPA\n",
    "# 16\tAmapá\tAP\n",
    "# 17\tTocantins\tTO\n",
    "# 21\tMaranhão\tMA\n",
    "# 22\tPiauí\tPI\n",
    "# 23\tCeará\tCE\n",
    "# 24\tRio Grande do Norte\tRN\n",
    "# 25\tParaíba\tPB\n",
    "# 26\tPernambuco\tPE\n",
    "# 27\tAlagoas\tAL\n",
    "# 28\tSergipe\tSE\n",
    "# 29\tBahia\tBA\n",
    "# 31\tMinas Gerais\tMG\n",
    "# 32\tEspírito Santo\tES\n",
    "# 33\tRio de Janeiro\tRJ\n",
    "# 35\tSão Paulo\tSP\n",
    "# 41\tParaná\tPR\n",
    "# 42\tSanta Catarina\tSC\n",
    "# 43\tRio Grande do Sul (*)\tRS\n",
    "# 50\tMato Grosso do Sul\tMS\n",
    "# 51\tMato Grosso\tMT\n",
    "# 52\tGoiás\tGO\n",
    "# 53\tDistrito Federal\tDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map \"SG_UF_NOT\" value to the region of Brazil as a new one-hot column use these values:\n",
    "# \"Norte\": [11, 12, 13, 14, 15, 16, 17],\n",
    "# \"Nordeste\": [21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
    "# \"Centro-Oeste\": [50, 51, 52, 53],\n",
    "# \"Sudeste\": [31, 32, 33, 35],\n",
    "# \"Sul\": [41, 42, 43]\n",
    "\n",
    "X_train[\"REGION_NORTH\"] = X_train[\"SG_UF_NOT\"].isin([11, 12, 13, 14, 15, 16, 17]).astype(int)\n",
    "X_train[\"REGION_NORTHEAST\"] = X_train[\"SG_UF_NOT\"].isin([21, 22, 23, 24, 25, 26, 27, 28, 29]).astype(int)\n",
    "X_train[\"REGION_MIDWEST\"] = X_train[\"SG_UF_NOT\"].isin([50, 51, 52, 53]).astype(int)\n",
    "X_train[\"REGION_SOUTHEAST\"] = X_train[\"SG_UF_NOT\"].isin([31, 32, 33, 35]).astype(int)\n",
    "X_train[\"REGION_SOUTH\"] = X_train[\"SG_UF_NOT\"].isin([41, 42, 43]).astype(int)\n",
    "\n",
    "# Remove the \"SG_UF_NOT\" and \"ID_MUNICIP\" column from X_train\n",
    "X_train = X_train.drop(columns=[\"SG_UF_NOT\", \"ID_MUNICIP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values in each region in X_train:\n",
      "\n",
      "Region North:  1727\n",
      "Region Northeast:  7800\n",
      "Region Midwest:  1089\n",
      "Region Southeast:  5266\n",
      "Region South:  510\n"
     ]
    }
   ],
   "source": [
    "# Count values in each region\n",
    "print(\"\\nValues in each region in X_train:\\n\")\n",
    "print(\"Region North: \", X_train[\"REGION_NORTH\"].sum())\n",
    "print(\"Region Northeast: \", X_train[\"REGION_NORTHEAST\"].sum())\n",
    "print(\"Region Midwest: \", X_train[\"REGION_MIDWEST\"].sum())\n",
    "print(\"Region Southeast: \", X_train[\"REGION_SOUTHEAST\"].sum())\n",
    "print(\"Region South: \", X_train[\"REGION_SOUTH\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of SEM_NOT column:  int64\n",
      "Type of NU_ANO column:  int64\n",
      "Min value of SEM_NOT column:  201801\n",
      "Max value of SEM_NOT column:  202252\n"
     ]
    }
   ],
   "source": [
    "# Use week and year info to create a new TIME column replacing DT_NOTIFIC, SEM_NOT and NU_ANO\n",
    "print(\"Type of SEM_NOT column: \", X_train[\"SEM_NOT\"].dtype)\n",
    "print(\"Type of NU_ANO column: \", X_train[\"NU_ANO\"].dtype)\n",
    "print(\"Min value of SEM_NOT column: \", X_train[\"SEM_NOT\"].min())\n",
    "print(\"Max value of SEM_NOT column: \", X_train[\"SEM_NOT\"].max())\n",
    "\n",
    "# Calculate the week and year from the SEM_NOT and NU_ANO columns\n",
    "week = X_train[\"SEM_NOT\"] % 100\n",
    "year = X_train[\"NU_ANO\"] - 2000 - start_year\n",
    "X_train[\"TIME\"] = year * 52 + week\n",
    "\n",
    "# Remove DT_NOTIFIC, SEM_NOT and NU_ANO columns\n",
    "X_train = X_train.drop(columns=[\"DT_NOTIFIC\", \"SEM_NOT\", \"NU_ANO\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in 'CLASSI_FIN' column:  CLASSI_FIN\n",
      "5.0     9149\n",
      "13.0    7182\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check all values in column \"CLASSI_FIN\"\n",
    "# 5.0 discarded\n",
    "# 13.0 confirmed case of Chikungunya\n",
    "print(\"Values in 'CLASSI_FIN' column: \", X_train[\"CLASSI_FIN\"].value_counts())\n",
    "\n",
    "# Map \"CLASSI_FIN\" value to 1 for confirmed cases and 0 for discarded cases\n",
    "X_train[\"CLASSI_FIN\"] = X_train[\"CLASSI_FIN\"].map({13.0: 1, 5.0: 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Not using SMOTE yet, need to clean the dataset columns first\n",
    "# # Set SMOTE sampling strategy:\n",
    "# # We want the minority class to be 10% of the total training data.\n",
    "# # Let r be the ratio minority/majority after resampling, then minority fraction = r/(1+r).\n",
    "# # For a 10% minority fraction, r = 0.10 / (1 - 0.10) ≈ 0.1111.\n",
    "# smote = SMOTE(sampling_strategy=0.1111, random_state=42)\n",
    "\n",
    "# # Apply SMOTE oversampling to the training data only\n",
    "# X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Print the new distribution for y_train\n",
    "# print(\"\\nNew y_train distribution after SMOTE oversampling:\")\n",
    "# print(y_train_smote.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the filtered data to a new CSV file in the cleaned_path directory\n",
    "\"\"\"\n",
    "\n",
    "# Save the filtered data to a new CSV file in the cleaned_path directory\n",
    "X_train.to_csv(f'{cleaned_path}X_train.csv', index=False)\n",
    "y_train.to_csv(f'{cleaned_path}y_train.csv', index=False)\n",
    "\n",
    "X_test.to_csv(f'{cleaned_path}X_test.csv', index=False)\n",
    "y_test.to_csv(f'{cleaned_path}y_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
